{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Logistic Regression\n",
        "##Assignment Questions"
      ],
      "metadata": {
        "id": "b2eksIm6_qBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical"
      ],
      "metadata": {
        "id": "odJaO_xD_p-0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear Regression.\n",
        "- **Logistic Regression** and **Linear Regression** are both supervised learning algorithms used in statistics and machine learning, but they are used for different types of problems and have different mathematical foundations.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **What is Logistic Regression?**\n",
        "\n",
        "**Logistic Regression** is used for **classification** problems ‚Äî most commonly **binary classification** (e.g., spam or not spam, yes or no, disease or no disease).\n",
        "\n",
        "* It predicts the **probability** that a given input point belongs to a certain class.\n",
        "* Instead of outputting a continuous value like linear regression, logistic regression outputs a value between 0 and 1 using the **sigmoid function**.\n",
        "\n",
        "#### ‚úÖ Key Formula:\n",
        "\n",
        "$$\n",
        "P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **What is Linear Regression?**\n",
        "\n",
        "**Linear Regression** is used for **regression** problems ‚Äî predicting a **continuous** outcome (e.g., price, temperature, sales).\n",
        "\n",
        "* It models the relationship between the dependent variable $y$ and one or more independent variables $x$ by fitting a linear equation.\n",
        "\n",
        "#### ‚úÖ Key Formula:\n",
        "\n",
        "$$\n",
        "y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n + \\epsilon\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ **Key Differences Between Logistic and Linear Regression**\n",
        "\n",
        "| Feature               | Linear Regression                   | Logistic Regression                      |\n",
        "| --------------------- | ----------------------------------- | ---------------------------------------- |\n",
        "| **Type of Output**    | Continuous (real number)            | Probability (between 0 and 1)            |\n",
        "| **Type of Problem**   | Regression                          | Classification                           |\n",
        "| **Algorithm Output**  | A value from $-\\infty$ to $+\\infty$ | A probability mapped through sigmoid     |\n",
        "| **Decision Boundary** | Not applicable                      | Threshold applied (e.g., 0.5 for binary) |\n",
        "| **Loss Function**     | Mean Squared Error (MSE)            | Log Loss (Cross-Entropy)                 |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ö†Ô∏è Misuse Alert\n",
        "\n",
        "Using **Linear Regression** for classification (by applying a threshold) often leads to poor performance, because it doesn't model probabilities correctly and can predict values outside the $[0,1]$ range.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Kb-gfqPM_p7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the mathematical equation of Logistic Regression.\n",
        "-### üîπ **Mathematical Equation of Logistic Regression**\n",
        "\n",
        "The goal of **Logistic Regression** is to model the **probability** that a binary outcome $y \\in \\{0, 1\\}$ occurs, given a set of input features $x = (x_1, x_2, ..., x_n)$.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. Linear Combination (Logit Function)**\n",
        "\n",
        "Logistic Regression first computes a **linear combination** of input features:\n",
        "\n",
        "$$\n",
        "z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_n x_n\n",
        "$$\n",
        "\n",
        "Here:\n",
        "\n",
        "* $\\beta_0$ is the **intercept** (bias term),\n",
        "* $\\beta_1, ..., \\beta_n$ are the **weights** (coefficients),\n",
        "* $x_1, ..., x_n$ are the input features.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Sigmoid Function (Logistic Function)**\n",
        "\n",
        "To convert the linear output $z$ into a **probability** between 0 and 1, apply the **sigmoid** function:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "So the predicted **probability** that $y = 1$ is:\n",
        "\n",
        "$$\n",
        "P(y=1|x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n)}}\n",
        "$$\n",
        "\n",
        "And:\n",
        "\n",
        "$$\n",
        "P(y=0|x) = 1 - P(y=1|x)\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. Decision Rule**\n",
        "\n",
        "To make a prediction:\n",
        "\n",
        "$$\n",
        "\\hat{y} =\n",
        "\\begin{cases}\n",
        "1 & \\text{if } P(y=1|x) \\geq 0.5 \\\\\n",
        "0 & \\text{if } P(y=1|x) < 0.5\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "CwCq_Ro__peC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Why do we use the Sigmoid function in Logistic Regression.\n",
        "\n",
        "-We use the **sigmoid function** in logistic regression because it transforms the **linear output** (which can range from $-\\infty$ to $+\\infty$) into a **probability** between **0 and 1**, which is essential for classification.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ What is the Sigmoid Function?\n",
        "\n",
        "The **sigmoid (logistic)** function is defined as:\n",
        "\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1 + e^{-z}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $z = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n$ is the linear combination of input features.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why the Sigmoid Function is Used\n",
        "\n",
        "| Reason                                       | Explanation                                                                                                                                               |               |                                                                                                                              |\n",
        "| -------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- | ---------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **1. Probability Output**                    | Logistic regression needs to model the **probability** that an input belongs to a class. The sigmoid function squashes the output into the \\[0, 1] range. |               |                                                                                                                              |\n",
        "| **2. Smooth and Differentiable**             | The sigmoid is smooth and differentiable, which is crucial for optimization (like using gradient descent).                                                |               |                                                                                                                              |\n",
        "| **3. Natural Fit for Binary Classification** | It models the **log-odds** of the event as a linear function:  (\\log\\left(\\frac{P(y=1                                                                     | x)}{1 - P(y=1 | x)}\\right) = \\beta\\_0 + \\beta\\_1 x\\_1 + \\dots + \\beta\\_n x\\_n). This log-odds interpretation is intuitive in classification. |\n",
        "| **4. Maps Linear to Non-linear**             | Converts the linear decision boundary into a nonlinear S-shaped curve, making it suitable for classification.                                             |               |                                                                                                                              |\n",
        "\n",
        "---\n",
        "\n",
        "### üìä Visual Insight\n",
        "\n",
        "The sigmoid curve:\n",
        "\n",
        "* Approaches **0** as $z \\to -\\infty$,\n",
        "* Approaches **1** as $z \\to +\\infty$,\n",
        "* Is centered at **0.5** when $z = 0$.\n",
        "\n",
        "This shape makes it ideal for deciding between two classes.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EjqancTT_pbK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the cost function of Logistic Regression.\n",
        "-### üîπ **Cost Function of Logistic Regression**\n",
        "\n",
        "In **logistic regression**, we use a cost function that measures how well the model's predicted probabilities match the actual binary labels (0 or 1). The standard cost function is based on **log-likelihood** and is called the **binary cross-entropy** or **log loss**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. Hypothesis Function Recap**\n",
        "\n",
        "The predicted probability that $y = 1$ given input $x$:\n",
        "\n",
        "$$\n",
        "\\hat{y} = h_\\theta(x) = \\frac{1}{1 + e^{-\\theta^T x}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\theta$ are the model parameters (weights and bias),\n",
        "* $x$ is the input vector.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Cost Function (Log Loss)**\n",
        "\n",
        "For a **single training example**:\n",
        "\n",
        "$$\n",
        "\\text{Cost}(\\hat{y}, y) = -y \\log(\\hat{y}) - (1 - y) \\log(1 - \\hat{y})\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $y \\in \\{0, 1\\}$ is the true label,\n",
        "* $\\hat{y}$ is the predicted probability.\n",
        "\n",
        "This formula:\n",
        "\n",
        "* Penalizes the model heavily when it confidently predicts the wrong class,\n",
        "* Is **convex**, which is ideal for optimization.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. Full Cost Function for the Dataset**\n",
        "\n",
        "For $m$ training examples:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log(\\hat{y}^{(i)}) - (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\hat{y}^{(i)} = h_\\theta(x^{(i)})$,\n",
        "* $y^{(i)}$ is the true label for example $i$.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Why This Cost Function?**\n",
        "\n",
        "* Derived from **maximum likelihood estimation (MLE)**.\n",
        "* Ensures the model learns to output probabilities close to the actual labels.\n",
        "* Unlike mean squared error (used in linear regression), **log loss** is better suited for classification because it punishes confident wrong predictions more severely.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Ap-vDLFc_pXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is Regularization in Logistic Regression? Why is it needed.\n",
        "\n",
        "- ### üîπ What is **Regularization** in Logistic Regression?\n",
        "\n",
        "**Regularization** is a technique used to prevent **overfitting** by adding a **penalty term** to the cost function in logistic regression. It discourages the model from fitting the training data **too perfectly**, which often leads to poor performance on unseen (test) data.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Why is Regularization Needed?\n",
        "\n",
        "| Problem                     | Explanation                                                                                                                                  |\n",
        "| --------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------- |\n",
        "| **Overfitting**             | When a model becomes too complex (e.g., too many features or large weights), it starts memorizing the training data instead of generalizing. |\n",
        "| **Noise Sensitivity**       | Without regularization, logistic regression might fit to noise or irrelevant features.                                                       |\n",
        "| **Improved Generalization** | Regularization helps the model generalize better by keeping weights small and simplifying the decision boundary.                             |\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ How Regularization Works\n",
        "\n",
        "In logistic regression, regularization **adds a penalty term** to the cost function that increases with the **magnitude of the coefficients**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ Types of Regularization\n",
        "\n",
        "#### 1. **L2 Regularization (Ridge)**\n",
        "\n",
        "* Adds a penalty equal to the **sum of squared weights**.\n",
        "* New cost function:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\left[ -y^{(i)} \\log(\\hat{y}^{(i)}) - (1 - y^{(i)}) \\log(1 - \\hat{y}^{(i)}) \\right] + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\lambda$ is the regularization strength (hyperparameter),\n",
        "* $\\theta_j$ are the model parameters (excluding bias).\n",
        "\n",
        "L2 encourages smaller, but non-zero weights.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **L1 Regularization (Lasso)**\n",
        "\n",
        "* Adds a penalty equal to the **sum of absolute values of weights**:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Log Loss} + \\frac{\\lambda}{m} \\sum_{j=1}^{n} |\\theta_j|\n",
        "$$\n",
        "\n",
        "L1 can shrink some weights to **zero**, effectively doing **feature selection**.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "| Concept                  | Logistic Regression Without Regularization | Logistic Regression With Regularization |\n",
        "| ------------------------ | ------------------------------------------ | --------------------------------------- |\n",
        "| **Risk**                 | Overfits easily if too many features       | Controls model complexity               |\n",
        "| **Weights**              | Can become large                           | Penalized for being too large           |\n",
        "| **Model Generalization** | Poor on new data                           | Better on unseen data                   |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MQtWCV3GBkKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Explain the difference between Lasso, Ridge, and Elastic Net regressionC\n",
        "- ### üîπ Difference Between **Lasso**, **Ridge**, and **Elastic Net** Regression\n",
        "\n",
        "These are all **regularization techniques** used in regression (including logistic regression and linear regression) to **prevent overfitting** and **improve generalization** by penalizing large coefficients.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 1. **Ridge Regression (L2 Regularization)**\n",
        "\n",
        "* **Penalty Term**: Sum of squares of the coefficients.\n",
        "* **Cost Function**:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Loss} + \\lambda \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "* **Effect**:\n",
        "\n",
        "  * Shrinks coefficients smoothly toward zero.\n",
        "  * **Does not eliminate** any coefficients (i.e., all features remain in the model).\n",
        "  * Good when **many features are correlated**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **Lasso Regression (L1 Regularization)**\n",
        "\n",
        "* **Penalty Term**: Sum of the **absolute values** of the coefficients.\n",
        "* **Cost Function**:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Loss} + \\lambda \\sum_{j=1}^{n} |\\theta_j|\n",
        "$$\n",
        "\n",
        "* **Effect**:\n",
        "\n",
        "  * Can **shrink some coefficients to exactly zero**.\n",
        "  * Performs **feature selection** ‚Äî helps make models simpler and more interpretable.\n",
        "  * Can be unstable when features are highly correlated.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 3. **Elastic Net Regression**\n",
        "\n",
        "* **Penalty Term**: Combination of L1 and L2 penalties.\n",
        "* **Cost Function**:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Loss} + \\lambda_1 \\sum_{j=1}^{n} |\\theta_j| + \\lambda_2 \\sum_{j=1}^{n} \\theta_j^2\n",
        "$$\n",
        "\n",
        "* Often written as:\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Loss} + \\lambda \\left[ \\alpha \\sum_{j=1}^{n} |\\theta_j| + (1 - \\alpha) \\sum_{j=1}^{n} \\theta_j^2 \\right]\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $\\lambda$ controls overall regularization strength,\n",
        "\n",
        "* $\\alpha \\in [0,1]$ balances between Lasso and Ridge.\n",
        "\n",
        "* **Effect**:\n",
        "\n",
        "  * Combines benefits of both Lasso (feature selection) and Ridge (stability with correlated features).\n",
        "  * Often preferred in practice, especially when the number of features is large.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary Table\n",
        "\n",
        "| Method          | Penalty Type | Can Set Coefficients to Zero? | Best Use Case                             |\n",
        "| --------------- | ------------ | ----------------------------- | ----------------------------------------- |\n",
        "| **Ridge**       | L2           | ‚ùå No                          | When features are correlated              |\n",
        "| **Lasso**       | L1           | ‚úÖ Yes                         | For feature selection (sparse models)     |\n",
        "| **Elastic Net** | L1 + L2      | ‚úÖ Yes                         | When you want both stability and sparsity |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2KBQWKRSBzTC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should we use Elastic Net instead of Lasso or Ridge.\n",
        "- ### üîπ When Should You Use **Elastic Net** Instead of **Lasso** or **Ridge**?\n",
        "\n",
        "**Elastic Net** is most useful when you want to combine the strengths of both **Lasso (L1)** and **Ridge (L2)** regularization. Here's when to prefer it:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Use Elastic Net When:**\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **You Have Many Features (High-Dimensional Data)**\n",
        "\n",
        "* Especially when the number of features $p$ is **greater than** the number of samples $n$ ‚Äî e.g., text data, gene expression data.\n",
        "* Lasso tends to select only **one** feature from a group of correlated features; Elastic Net can include **groups** of correlated features.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Your Features Are Highly Correlated**\n",
        "\n",
        "* Lasso may arbitrarily pick one correlated feature and ignore others.\n",
        "* Ridge handles correlated features better, but doesn't perform feature selection.\n",
        "* **Elastic Net blends both**: it selects features (like Lasso) but also shares weights across correlated groups (like Ridge).\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **You Want Sparse Models, But Lasso Alone Is Too Aggressive**\n",
        "\n",
        "* Lasso may **zero out too many features**, especially if they‚Äôre moderately informative but correlated.\n",
        "* Elastic Net allows **more nuanced sparsity**, retaining important signals that Lasso might discard.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **You're Unsure Whether to Choose Lasso or Ridge**\n",
        "\n",
        "* Elastic Net includes a mixing parameter $\\alpha \\in [0,1]$:\n",
        "\n",
        "  * $\\alpha = 1$ ‚Üí Lasso\n",
        "  * $\\alpha = 0$ ‚Üí Ridge\n",
        "  * $0 < \\alpha < 1$ ‚Üí Elastic Net (a mix)\n",
        "\n",
        "Tuning $\\alpha$ via cross-validation lets the model **find the best balance** automatically.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary\n",
        "\n",
        "| Situation                              | Recommended Approach |\n",
        "| -------------------------------------- | -------------------- |\n",
        "| Few features, some irrelevant          | Lasso                |\n",
        "| Many correlated features               | Ridge or Elastic Net |\n",
        "| Need for feature selection + stability | **Elastic Net**      |\n",
        "| High-dimensional data                  | **Elastic Net**      |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Q-OMMXVKBzPq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the impact of the regularization parameter (Œª) in Logistic Regression.\n",
        "\n",
        "-### üîπ Impact of the Regularization Parameter (Œª) in Logistic Regression\n",
        "\n",
        "The **regularization parameter** $\\lambda$ (lambda) in logistic regression controls the **strength of the penalty** applied to the model‚Äôs coefficients. It's a key hyperparameter that directly affects the **complexity**, **performance**, and **generalization ability** of the model.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **What Œª Does:**\n",
        "\n",
        "$$\n",
        "J(\\theta) = \\text{Log Loss} + \\lambda \\cdot \\text{Penalty}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* Penalty is $\\sum \\theta_j^2$ for Ridge (L2), or $\\sum |\\theta_j|$ for Lasso (L1), or a mix for Elastic Net.\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ **Effects of Different Œª Values:**\n",
        "\n",
        "| Œª Value         | Effect on Model                                                               |\n",
        "| --------------- | ----------------------------------------------------------------------------- |\n",
        "| **Œª = 0**       | No regularization ‚Üí model may overfit (memorizes noise in training data)      |\n",
        "| **Small Œª**     | Light regularization ‚Üí allows more flexibility, may still overfit slightly    |\n",
        "| **Moderate Œª**  | Balanced regularization ‚Üí helps prevent overfitting while preserving accuracy |\n",
        "| **Large Œª**     | Heavy regularization ‚Üí shrinks weights aggressively, may underfit the data    |\n",
        "| **Too large Œª** | Model becomes too simple ‚Üí might ignore useful features and perform poorly    |\n",
        "\n",
        "---\n",
        "\n",
        "### üî∏ **Visual Insight:**\n",
        "\n",
        "* Increasing $\\lambda$ **shrinks** coefficients toward **zero**.\n",
        "* This leads to **simpler models** (smaller weights), which usually generalize better.\n",
        "* However, shrinking too much can result in a model that **fails to capture patterns** (underfitting).\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Rule of Thumb:\n",
        "\n",
        "* **Start small** and use **cross-validation** to choose the best Œª.\n",
        "* A **grid search** or **regularization path** (as in `scikit-learn`) helps tune Œª effectively.\n",
        "\n",
        "---\n",
        "\n",
        "### üß™ Example Use in Python (scikit-learn):\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# C = 1/Œª in scikit-learn\n",
        "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}  # Smaller C = stronger regularization\n",
        "model = GridSearchCV(LogisticRegression(penalty='l2', solver='liblinear'), param_grid)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NFtVEGRJBzNa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What are the key assumptions of Logistic Regression.\n",
        "- ### üîπ Key Assumptions of Logistic Regression\n",
        "\n",
        "Although **logistic regression** is more flexible than linear regression, it still relies on several key assumptions to produce reliable and interpretable results.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 1. **Binary or Categorical Outcome**\n",
        "\n",
        "* The **dependent variable** (target) must be **binary** (0 or 1) for binary logistic regression.\n",
        "* For **multinomial logistic regression**, the outcome can have more than two categories.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **Linear Relationship with the Logit**\n",
        "\n",
        "* Logistic regression **does not assume a linear relationship** between the independent variables and the outcome.\n",
        "* However, it **does assume** a **linear relationship between the independent variables and the log-odds** (logit) of the outcome:\n",
        "\n",
        "$$\n",
        "\\log\\left(\\frac{P(y=1)}{1 - P(y=1)}\\right) = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_n x_n\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 3. **Independent Observations**\n",
        "\n",
        "* Observations should be **independent** of each other.\n",
        "* Violating this (e.g., with repeated measures or time series data) can bias the results and standard errors.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 4. **No or Little Multicollinearity**\n",
        "\n",
        "* Independent variables should **not be highly correlated** with each other.\n",
        "* High multicollinearity makes coefficient estimates unstable and hard to interpret.\n",
        "\n",
        "‚úÖ Tip: Use **Variance Inflation Factor (VIF)** to detect multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 5. **Large Sample Size**\n",
        "\n",
        "* Logistic regression requires **a sufficient number of observations**, especially for rare events.\n",
        "* The model estimates probabilities, so small datasets can produce **unstable and biased estimates**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 6. **No Strong Outliers**\n",
        "\n",
        "* Outliers can strongly influence logistic regression results, especially in smaller datasets.\n",
        "* Use influence diagnostics (like Cook‚Äôs distance) to detect problematic points.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 7. **Independent Variables Are Not Required to Be Normally Distributed**\n",
        "\n",
        "* Unlike linear regression, logistic regression **does not require normality, homoscedasticity, or equal variance** of predictors.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Summary Table\n",
        "\n",
        "| Assumption                        | Required in Logistic Regression |\n",
        "| --------------------------------- | ------------------------------- |\n",
        "| Binary or categorical outcome     | ‚úÖ Yes                           |\n",
        "| Linear relationship with logit    | ‚úÖ Yes                           |\n",
        "| Independent observations          | ‚úÖ Yes                           |\n",
        "| No multicollinearity              | ‚úÖ Yes                           |\n",
        "| Large sample size                 | ‚úÖ Recommended                   |\n",
        "| Normal distribution of predictors | ‚ùå No                            |\n",
        "| Equal variance of predictors      | ‚ùå No                            |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "czdD6nyXBzKy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are some alternatives to Logistic Regression for classification tasks.\n",
        "- ### üîπ Alternatives to Logistic Regression for Classification Tasks\n",
        "\n",
        "While **logistic regression** is a strong baseline for classification problems, there are many other models that can offer better performance in certain situations ‚Äî especially with **nonlinear relationships**, **complex feature interactions**, or **high-dimensional data**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 1. **Decision Trees**\n",
        "\n",
        "* Simple, interpretable models that split the data into decision rules.\n",
        "* **Pros**: Easy to interpret, handles nonlinearity and feature interactions.\n",
        "* **Cons**: Prone to overfitting unless pruned.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 2. **Random Forest**\n",
        "\n",
        "* An ensemble of decision trees trained on different data subsets and features.\n",
        "* **Pros**: Reduces overfitting, handles missing values, works well with many features.\n",
        "* **Cons**: Less interpretable, slower than logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 3. **Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost)**\n",
        "\n",
        "* Build trees sequentially to correct the errors of previous trees.\n",
        "* **Pros**: Excellent predictive performance, handles complex patterns.\n",
        "* **Cons**: Sensitive to hyperparameters, less interpretable.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 4. **Support Vector Machines (SVM)**\n",
        "\n",
        "* Finds the optimal hyperplane that separates classes with the largest margin.\n",
        "* **Pros**: Effective in high-dimensional spaces, works well with clear margins.\n",
        "* **Cons**: Computationally intensive, hard to interpret, sensitive to parameter tuning.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 5. **k-Nearest Neighbors (k-NN)**\n",
        "\n",
        "* Classifies based on the majority label of the k closest training examples.\n",
        "* **Pros**: Simple, no training time.\n",
        "* **Cons**: Slow at prediction time, sensitive to the choice of $k$ and distance metric.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 6. **Naive Bayes**\n",
        "\n",
        "* Based on Bayes‚Äô theorem with strong independence assumptions.\n",
        "* **Pros**: Fast, works well with text (e.g., spam detection), easy to implement.\n",
        "* **Cons**: Assumes feature independence, which may not hold in practice.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 7. **Neural Networks (Deep Learning)**\n",
        "\n",
        "* Models that learn complex, nonlinear functions using layers of neurons.\n",
        "* **Pros**: Powerful for large and complex datasets (images, text, etc.).\n",
        "* **Cons**: Requires large data, longer training time, less interpretable.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ 8. **Linear Discriminant Analysis (LDA)**\n",
        "\n",
        "* Projects data to maximize class separability.\n",
        "* **Pros**: Good for linear boundaries, works well with small sample sizes.\n",
        "* **Cons**: Assumes normally distributed features with equal variance.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Model Comparison Summary\n",
        "\n",
        "| Model               | Handles Nonlinearity | Interpretable | Good for Small Data | High Accuracy Potential |\n",
        "| ------------------- | -------------------- | ------------- | ------------------- | ----------------------- |\n",
        "| Logistic Regression | ‚ùå Linear only        | ‚úÖ Yes         | ‚úÖ Yes               | ‚ö†Ô∏è Moderate             |\n",
        "| Decision Tree       | ‚úÖ Yes                | ‚úÖ Yes         | ‚úÖ Yes               | ‚ö†Ô∏è Risk of overfitting  |\n",
        "| Random Forest       | ‚úÖ Yes                | ‚ùå Limited     | ‚úÖ Yes               | ‚úÖ High                  |\n",
        "| XGBoost/LightGBM    | ‚úÖ Yes                | ‚ùå No          | ‚úÖ Yes               | ‚úÖ Very High             |\n",
        "| SVM                 | ‚úÖ Yes                | ‚ö†Ô∏è Limited    | ‚ö†Ô∏è Not ideal        | ‚úÖ High                  |\n",
        "| k-NN                | ‚úÖ Yes                | ‚úÖ Simple      | ‚ö†Ô∏è Slow prediction  | ‚ö†Ô∏è Moderate             |\n",
        "| Naive Bayes         | ‚ùå Limited            | ‚úÖ Yes         | ‚úÖ Yes               | ‚úÖ With text data        |\n",
        "| Neural Networks     | ‚úÖ Very               | ‚ùå No          | ‚ùå Needs large data  | ‚úÖ Very High             |\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "ZB6HfTUIC8Ov"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. BBC What are Classification Evaluation Metrics.\n",
        "- ### üîπ **Classification Evaluation Metrics**\n",
        "\n",
        "When evaluating the performance of classification models, various **metrics** are used to measure how well the model predicts the classes. These metrics are crucial for understanding model accuracy, robustness, and potential biases. Here are the key **evaluation metrics** used in classification:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. Accuracy**\n",
        "\n",
        "* **Definition**: The proportion of correctly predicted instances (both positives and negatives) over all instances.\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total Instances}}\n",
        "$$\n",
        "\n",
        "* **Pros**: Simple to understand and interpret.\n",
        "* **Cons**: May not be informative when the dataset is imbalanced (e.g., many more negatives than positives).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Precision (Positive Predictive Value)**\n",
        "\n",
        "* **Definition**: The proportion of positive predictions that are actually correct. It answers the question: \"Out of all the instances predicted as positive, how many were truly positive?\"\n",
        "\n",
        "$$\n",
        "\\text{Precision} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}\n",
        "$$\n",
        "\n",
        "* **Pros**: Important when the cost of false positives is high (e.g., predicting a disease when the person doesn't have it).\n",
        "* **Cons**: Precision alone can be misleading in imbalanced datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. Recall (Sensitivity or True Positive Rate)**\n",
        "\n",
        "* **Definition**: The proportion of actual positive instances that were correctly predicted as positive. It answers the question: \"Out of all the true positives, how many did we capture?\"\n",
        "\n",
        "$$\n",
        "\\text{Recall} = \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}\n",
        "$$\n",
        "\n",
        "* **Pros**: Crucial when the cost of false negatives is high (e.g., missing out on detecting a disease).\n",
        "* **Cons**: May result in low precision if the model is too lenient in predicting positives.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **4. F1-Score**\n",
        "\n",
        "* **Definition**: The harmonic mean of **precision** and **recall**. It balances both metrics, providing a single score to evaluate the model.\n",
        "\n",
        "$$\n",
        "F1\\text{-Score} = 2 \\times \\frac{\\text{Precision} \\times \\text{Recall}}{\\text{Precision} + \\text{Recall}}\n",
        "$$\n",
        "\n",
        "* **Pros**: Useful when you need a balance between precision and recall, especially in imbalanced datasets.\n",
        "* **Cons**: Can be hard to interpret on its own without knowing the precision and recall values.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **5. ROC Curve (Receiver Operating Characteristic Curve)**\n",
        "\n",
        "* **Definition**: A graphical plot that shows the trade-off between **True Positive Rate (Recall)** and **False Positive Rate** at different classification thresholds.\n",
        "\n",
        "  * **X-axis**: False Positive Rate (FPR) = $\\frac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}$\n",
        "  * **Y-axis**: True Positive Rate (Recall) = $\\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$\n",
        "\n",
        "* **AUC (Area Under the Curve)**:\n",
        "\n",
        "  * **AUC** measures the overall performance of the model. A higher AUC indicates better performance.\n",
        "  * **Perfect classifier**: AUC = 1, **Random classifier**: AUC = 0.5.\n",
        "\n",
        "* **Pros**: Good for understanding the model performance across various thresholds.\n",
        "\n",
        "* **Cons**: Can be less intuitive than metrics like accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **6. Confusion Matrix**\n",
        "\n",
        "* **Definition**: A table used to describe the performance of a classification model. It breaks down the predictions into:\n",
        "\n",
        "  |                     | Predicted Positive  | Predicted Negative  |\n",
        "  | ------------------- | ------------------- | ------------------- |\n",
        "  | **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
        "  | **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
        "\n",
        "* **Pros**: Gives a complete picture of model performance, including false positives and false negatives.\n",
        "\n",
        "* **Cons**: Harder to interpret without other metrics.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **7. Specificity (True Negative Rate)**\n",
        "\n",
        "* **Definition**: The proportion of actual negative instances that were correctly identified as negative. It answers: \"Out of all the true negatives, how many did we catch?\"\n",
        "\n",
        "$$\n",
        "\\text{Specificity} = \\frac{\\text{True Negatives}}{\\text{True Negatives} + \\text{False Positives}}\n",
        "$$\n",
        "\n",
        "* **Pros**: Important in cases where the cost of false positives is high.\n",
        "* **Cons**: Rarely used on its own and usually considered along with sensitivity.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **8. Log Loss (Logarithmic Loss or Cross-Entropy Loss)**\n",
        "\n",
        "* **Definition**: Measures the uncertainty of your predictions based on the probability values output by the model. It calculates the log loss for each instance and averages it across all instances.\n",
        "\n",
        "$$\n",
        "\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ y_i \\log(p_i) + (1 - y_i) \\log(1 - p_i) \\right]\n",
        "$$\n",
        "\n",
        "Where $y_i$ is the true label and $p_i$ is the predicted probability of the positive class.\n",
        "\n",
        "* **Pros**: Useful for probabilistic models and works well when evaluating classification models that output probabilities.\n",
        "* **Cons**: Not as intuitive as accuracy or precision/recall.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **When to Use Each Metric**\n",
        "\n",
        "| Metric               | Best Used When:                                                     |\n",
        "| -------------------- | ------------------------------------------------------------------- |\n",
        "| **Accuracy**         | Data is balanced, and you care about overall performance.           |\n",
        "| **Precision**        | False positives are costly (e.g., in spam detection).               |\n",
        "| **Recall**           | False negatives are costly (e.g., in disease detection).            |\n",
        "| **F1-Score**         | You need a balance between precision and recall.                    |\n",
        "| **ROC/AUC**          | You need to compare models or assess performance across thresholds. |\n",
        "| **Confusion Matrix** | You need to understand the breakdown of TP, TN, FP, FN.             |\n",
        "| **Specificity**      | False positives are important to minimize.                          |\n",
        "| **Log Loss**         | You care about the uncertainty in probabilistic predictions.        |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EIPps4GGC8LN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How does class imbalance affect Logistic Regression.\n",
        "- ### üîπ **How Class Imbalance Affects Logistic Regression**\n",
        "\n",
        "**Class imbalance** occurs when the classes in your dataset are not represented equally. For example, in a binary classification task, if 95% of the instances belong to the negative class and only 5% to the positive class, the dataset is considered imbalanced.\n",
        "\n",
        "In the context of **Logistic Regression**, class imbalance can have several significant impacts on the model's performance and its predictions. Let's explore these effects and potential solutions.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Impacts of Class Imbalance on Logistic Regression**\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Biased Predictions**\n",
        "\n",
        "* **Effect**: Logistic regression, like many classification models, is trained to minimize the overall error or cost (e.g., **log-loss** or **cross-entropy loss**). If the dataset is highly imbalanced, the model may **focus on predicting the majority class** correctly, because doing so will result in a lower cost.\n",
        "* **Result**: The model might predict the **majority class** (e.g., the negative class in a 95%/5% split) for most instances, leading to poor performance on the minority class.\n",
        "* **Example**: If 95% of the data points are negatives (0) and 5% are positives (1), the model could predict 0 for almost every instance, achieving high accuracy but failing to identify positive instances correctly.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Misleading Accuracy**\n",
        "\n",
        "* **Effect**: Accuracy can be a misleading metric when class imbalance is present.\n",
        "* **Example**: In a 95/5 imbalanced dataset, predicting only the majority class can yield **95% accuracy**, which sounds good but actually performs poorly in detecting the minority class.\n",
        "* **Solution**: Use metrics like **precision**, **recall**, **F1-score**, or **AUC-ROC**, which provide more insight into how the model handles the minority class.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Incorrect Model Probabilities**\n",
        "\n",
        "* **Effect**: Logistic regression outputs **probabilities** between 0 and 1. When class imbalance is present, the predicted probability for the minority class may become skewed, leading to **biased probabilities**.\n",
        "* **Result**: The model might produce probabilities close to 0 for the minority class, even for instances that should have a higher probability of being positive.\n",
        "* **Solution**: Adjusting the decision threshold or using techniques like **class weighting** can help to rebalance the predicted probabilities.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Solutions to Handle Class Imbalance in Logistic Regression**\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Resampling Techniques**\n",
        "\n",
        "* **Oversampling the Minority Class**: Increase the number of instances in the minority class by duplicating them or generating synthetic samples (e.g., using **SMOTE**).\n",
        "* **Undersampling the Majority Class**: Reduce the number of instances in the majority class to balance the dataset.\n",
        "* **Effect**: Balancing the classes helps the model learn more about the minority class, improving its ability to predict both classes accurately.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Class Weights**\n",
        "\n",
        "* **Effect**: Assign higher **weights** to the minority class to penalize the model more for incorrect predictions of the minority class. This encourages the model to pay more attention to the minority class.\n",
        "* **Implementation in Logistic Regression**:\n",
        "  In **scikit-learn**, you can set `class_weight='balanced'`, which automatically adjusts the weights inversely proportional to class frequencies.\n",
        "\n",
        "  ```python\n",
        "  from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "  model = LogisticRegression(class_weight='balanced')\n",
        "  model.fit(X_train, y_train)\n",
        "  ```\n",
        "* **Pros**: Simple to implement and helps improve performance for imbalanced datasets.\n",
        "* **Cons**: It may lead to an increased risk of **overfitting** if the class weights are too high.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Threshold Moving**\n",
        "\n",
        "* **Effect**: Logistic regression assigns a class based on a **probability threshold** (usually 0.5). You can **move** this threshold for better class balance. For example, instead of using 0.5, you could set a lower threshold for the minority class, which makes it easier for the model to predict a positive class.\n",
        "* **Example**: If the threshold for predicting a positive class is changed to 0.3 instead of 0.5, the model might predict positives more often, improving recall at the cost of precision.\n",
        "* **Pros**: Simple to implement and provides more control over the trade-off between precision and recall.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Use of Different Metrics**\n",
        "\n",
        "* **Effect**: In the presence of class imbalance, **accuracy** is not an adequate evaluation metric. Instead, use:\n",
        "\n",
        "  * **Precision, Recall, and F1-Score**: To focus on the performance of the minority class.\n",
        "  * **ROC-AUC**: To evaluate the model‚Äôs ability to discriminate between the classes across all possible thresholds.\n",
        "  * **Precision-Recall Curve**: Especially useful when dealing with imbalanced datasets.\n",
        "* **Pros**: These metrics provide a more accurate representation of the model‚Äôs ability to handle imbalanced classes.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Use of Ensemble Methods (e.g., Random Forest, XGBoost)**\n",
        "\n",
        "* **Effect**: Ensemble methods like **Random Forest** or **XGBoost** can help handle class imbalance more effectively by combining multiple weak models to create a strong one. These models are less sensitive to class imbalance.\n",
        "* **Pros**: Can provide better performance on imbalanced datasets.\n",
        "* **Cons**: Less interpretable than logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary of Key Points**\n",
        "\n",
        "| Effect                         | Explanation                                                         | Solution                                               |\n",
        "| ------------------------------ | ------------------------------------------------------------------- | ------------------------------------------------------ |\n",
        "| **Bias toward Majority Class** | Logistic regression may favor the majority class due to imbalance.  | Use **class weights** or **resampling**.               |\n",
        "| **Misleading Accuracy**        | High accuracy may be misleading in imbalanced data.                 | Use **F1-score**, **Precision**, **Recall**.           |\n",
        "| **Skewed Probabilities**       | The model might output biased probabilities for the minority class. | Use **threshold moving** or **class weights**.         |\n",
        "| **Model Performance**          | Logistic regression may underperform on the minority class.         | Use **SMOTE**, **undersampling**, or ensemble methods. |\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "jOENMrjhC8H4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. B$C What is Hyperparameter Tuning in Logistic Regression.\n",
        "- ### üîπ **Hyperparameter Tuning in Logistic Regression**\n",
        "\n",
        "**Hyperparameter tuning** refers to the process of selecting the most optimal values for the hyperparameters of a machine learning model. For **Logistic Regression**, hyperparameter tuning is crucial for improving the model's performance. While logistic regression itself is relatively simple, it has hyperparameters that can significantly impact its accuracy, robustness, and ability to generalize.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Key Hyperparameters in Logistic Regression**\n",
        "\n",
        "Here are some of the key hyperparameters to tune in **Logistic Regression**:\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Regularization Strength (C)**\n",
        "\n",
        "* **Definition**: The **regularization parameter** controls the **penalty** applied to the model‚Äôs coefficients to prevent overfitting. The regularization term is the inverse of **C**.\n",
        "* **Effect**:\n",
        "\n",
        "  * **Low values of C** (strong regularization) make the model simpler (more regularized).\n",
        "  * **High values of C** (weak regularization) allow the model to fit the training data more closely.\n",
        "* **Default**: `C=1.0`\n",
        "* **Range**: Common values for **C** are between 0.001 and 1000, depending on the data.\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Regularization Type (Penalty)**\n",
        "\n",
        "* **Definition**: Logistic Regression can use two types of regularization:\n",
        "\n",
        "  * **L1 (Lasso)**: Promotes sparsity, i.e., some coefficients become zero.\n",
        "  * **L2 (Ridge)**: Shrinks coefficients toward zero but doesn‚Äôt set them exactly to zero.\n",
        "  * **Elastic Net**: A combination of L1 and L2 regularization.\n",
        "* **Effect**:\n",
        "\n",
        "  * **L1** can be useful for feature selection as it can eliminate less important features.\n",
        "  * **L2** works well in most cases and is the default in `scikit-learn`.\n",
        "  * **Elastic Net** is helpful when dealing with high-dimensional data and correlations between features.\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Solver**\n",
        "\n",
        "* **Definition**: The algorithm used to optimize the cost function in logistic regression.\n",
        "* **Options**:\n",
        "\n",
        "  * **'liblinear'**: Good for small datasets. It supports both **L1** and **L2** regularization.\n",
        "  * **'saga'**: Works well with large datasets, supports **L1**, **L2**, and **Elastic Net**.\n",
        "  * **'newton-cg'**, **'lbfgs'**: Suitable for larger datasets and **L2** regularization.\n",
        "* **Effect**: Choosing the appropriate solver impacts the model's convergence speed and scalability.\n",
        "\n",
        "---\n",
        "\n",
        "#### 4. **Max Iterations (max\\_iter)**\n",
        "\n",
        "* **Definition**: The maximum number of iterations the solver will run to converge to the optimal solution.\n",
        "* **Effect**:\n",
        "\n",
        "  * If the solver has not converged within the specified number of iterations, increasing `max_iter` can help.\n",
        "  * Default: `max_iter=100`\n",
        "  * Tuning this is useful if the model is taking too long to converge.\n",
        "\n",
        "---\n",
        "\n",
        "#### 5. **Tolerance (tol)**\n",
        "\n",
        "* **Definition**: The tolerance for stopping criteria. When the change in the cost function between iterations is smaller than `tol`, the solver stops.\n",
        "* **Effect**: A lower tolerance means the model will run more iterations, possibly resulting in a more accurate model at the cost of longer computation.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Why Hyperparameter Tuning is Important**\n",
        "\n",
        "* **Model Performance**: Optimal hyperparameters help the model generalize well to unseen data, preventing overfitting or underfitting.\n",
        "* **Efficiency**: Properly tuned parameters ensure faster convergence and reduce computational time.\n",
        "* **Balance**: Tuning the regularization parameters, for example, can help strike a balance between bias and variance.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Common Hyperparameter Tuning Methods**\n",
        "\n",
        "Here are some common methods used for **hyperparameter tuning**:\n",
        "\n",
        "---\n",
        "\n",
        "#### 1. **Grid Search**\n",
        "\n",
        "* **Definition**: An exhaustive search over a predefined set of hyperparameters. For example, you could search over a grid of `C` values and regularization types.\n",
        "* **Pros**: Simple and thorough. You can search over combinations of hyperparameters.\n",
        "* **Cons**: Computationally expensive for large grids and high-dimensional hyperparameters.\n",
        "\n",
        "**Example (Python)**:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports L1 regularization\n",
        "}\n",
        "\n",
        "model = LogisticRegression()\n",
        "grid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 2. **Random Search**\n",
        "\n",
        "* **Definition**: Instead of searching every possible combination of hyperparameters, **random search** samples from a range of values for each hyperparameter. This can be more efficient than grid search, especially for large hyperparameter spaces.\n",
        "* **Pros**: Less computationally expensive than grid search and can yield good results with fewer evaluations.\n",
        "\n",
        "**Example (Python)**:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from scipy.stats import uniform\n",
        "\n",
        "# Define the parameter distribution\n",
        "param_dist = {\n",
        "    'C': uniform(0.01, 100),\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'solver': ['liblinear']  # 'liblinear' supports L1 regularization\n",
        "}\n",
        "\n",
        "model = LogisticRegression()\n",
        "random_search = RandomizedSearchCV(model, param_dist, n_iter=100, cv=5, scoring='accuracy', random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"Best Parameters:\", random_search.best_params_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "#### 3. **Bayesian Optimization**\n",
        "\n",
        "* **Definition**: A probabilistic model-based optimization method. It tries to find the best hyperparameters by modeling the performance function and selecting the next set of hyperparameters based on past performance.\n",
        "* **Pros**: More efficient than random or grid search for large and complex search spaces.\n",
        "* **Cons**: Computationally more expensive and requires specialized libraries (e.g., `Hyperopt`, `Optuna`).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Tuning Example: Optimizing Logistic Regression**\n",
        "\n",
        "Let‚Äôs consider an example where we tune the key hyperparameters of Logistic Regression:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Parameter grid to search over\n",
        "param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2'],       # Regularization type\n",
        "    'solver': ['liblinear'],       # Solver type\n",
        "    'max_iter': [100, 200, 300]    # Max iterations\n",
        "}\n",
        "\n",
        "# Logistic Regression model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Grid search with cross-validation\n",
        "grid_search = GridSearchCV(log_reg, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary**\n",
        "\n",
        "* **Hyperparameters** like regularization strength (`C`), penalty type, and solver are crucial in tuning the performance of Logistic Regression.\n",
        "* **Methods for Tuning**: Grid Search, Random Search, and Bayesian Optimization.\n",
        "* **Goal**: Improve model performance by finding the optimal balance between bias and variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "jpOE-uRbC8E_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are different solvers in Logistic Regression? Which one should be used.\n",
        "- ### üîπ **Solvers in Logistic Regression**\n",
        "\n",
        "In **Logistic Regression**, the solver refers to the optimization algorithm used to minimize the cost function (typically **log-loss** or **cross-entropy loss**) during training. Each solver has its own strengths, limitations, and suitable use cases. The choice of solver can impact the model's performance and convergence speed.\n",
        "\n",
        "Here‚Äôs an overview of the different solvers available in **Logistic Regression** (as implemented in **`scikit-learn`**) and guidance on when to use each one:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. liblinear**\n",
        "\n",
        "* **Definition**: The **liblinear** solver is a **coordinate descent-based algorithm** that handles both **L1** (Lasso) and **L2** (Ridge) regularization.\n",
        "* **Advantages**:\n",
        "\n",
        "  * **Works well for small datasets**.\n",
        "  * **Supports L1 regularization**, which is useful for feature selection (sparsity).\n",
        "  * **Fast** for small datasets.\n",
        "* **Limitations**:\n",
        "\n",
        "  * May not be as efficient for larger datasets compared to other solvers.\n",
        "* **When to Use**:\n",
        "\n",
        "  * When the dataset is **small** or **moderate** in size.\n",
        "  * When you need to use **L1 regularization** (e.g., feature selection or sparsity).\n",
        "  * Suitable for **binary classification** and **multi-class classification** (one-vs-rest).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. newton-cg**\n",
        "\n",
        "* **Definition**: **Newton's Conjugate Gradient (newton-cg)** is a **second-order optimization** algorithm that uses **Newton's method** to find the optimal parameters.\n",
        "* **Advantages**:\n",
        "\n",
        "  * **Efficient for large datasets** and works well with **L2 regularization**.\n",
        "  * **Faster convergence** for larger datasets.\n",
        "  * Well-suited for **multiclass classification** problems.\n",
        "* **Limitations**:\n",
        "\n",
        "  * **Does not support L1 regularization**.\n",
        "  * Computationally expensive in terms of memory, as it requires the computation of second-order derivatives.\n",
        "* **When to Use**:\n",
        "\n",
        "  * When you have **large datasets**.\n",
        "  * When you need **L2 regularization** (which is the default regularization type).\n",
        "  * When working with **multiclass classification** (more than two classes).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. lbfgs**\n",
        "\n",
        "* **Definition**: **Limited-memory Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno (LBFGS)** is an approximation of Newton's method that uses **limited memory** to store previous calculations, making it more memory-efficient.\n",
        "* **Advantages**:\n",
        "\n",
        "  * **Efficient for large datasets**.\n",
        "  * **Supports L2 regularization**.\n",
        "  * **Fast convergence** in many cases.\n",
        "  * Works well for **multiclass classification** problems.\n",
        "* **Limitations**:\n",
        "\n",
        "  * **Does not support L1 regularization**.\n",
        "  * Can struggle with datasets that have **large numbers of features** and may require more memory.\n",
        "* **When to Use**:\n",
        "\n",
        "  * When you have **large datasets** with **L2 regularization**.\n",
        "  * When you want the solver to converge quickly with **multiclass classification**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **4. saga**\n",
        "\n",
        "* **Definition**: **SAGA** is a **stochastic gradient descent** variant that is **efficient** and works well with **L1** regularization and **L2 regularization**.\n",
        "* **Advantages**:\n",
        "\n",
        "  * **Supports both L1 and L2 regularization**, making it versatile.\n",
        "  * **Handles large datasets** efficiently.\n",
        "  * Works well for datasets with **sparse features**.\n",
        "  * Particularly effective for **multiclass classification**.\n",
        "* **Limitations**:\n",
        "\n",
        "  * Can be **slower for small datasets** compared to solvers like **liblinear**.\n",
        "  * **Requires more tuning** in terms of learning rate and iteration settings.\n",
        "* **When to Use**:\n",
        "\n",
        "  * When you have **large datasets**.\n",
        "  * When you need **L1 regularization** (e.g., for feature selection).\n",
        "  * When the dataset is **sparse** (e.g., many zeros in the features).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **5. saga vs liblinear**\n",
        "\n",
        "* **SAGA** can be seen as an improvement over **liblinear** for **large datasets** with **L1 regularization**. It is often faster for large datasets and can handle both **L1** and **L2** regularization efficiently.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Comparison of Solvers in Logistic Regression**\n",
        "\n",
        "| Solver        | Regularization Support | Suitable for Large Datasets | Supports L1 Regularization | Supports Multiclass | Convergence Speed | Memory Usage | Best Use Case                                                |\n",
        "| ------------- | ---------------------- | --------------------------- | -------------------------- | ------------------- | ----------------- | ------------ | ------------------------------------------------------------ |\n",
        "| **liblinear** | L1, L2                 | No                          | Yes                        | Yes (One-vs-Rest)   | Medium            | Low          | Small datasets, binary classification, L1 regularization     |\n",
        "| **newton-cg** | L2                     | Yes                         | No                         | Yes                 | Fast              | High         | Large datasets, L2 regularization, multiclass classification |\n",
        "| **lbfgs**     | L2                     | Yes                         | No                         | Yes                 | Fast              | Medium       | Large datasets, L2 regularization, multiclass classification |\n",
        "| **saga**      | L1, L2                 | Yes                         | Yes                        | Yes                 | Medium            | Low          | Large datasets, sparse data, L1 or L2 regularization         |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Which Solver to Use?**\n",
        "\n",
        "* **Use `liblinear`**:\n",
        "\n",
        "  * When you have **small datasets** or moderate-sized datasets.\n",
        "  * When you need **L1 regularization** for feature selection or sparsity.\n",
        "  * For **binary classification** or **multi-class classification** (with one-vs-rest).\n",
        "\n",
        "* **Use `newton-cg` or `lbfgs`**:\n",
        "\n",
        "  * When you have **large datasets** and need **L2 regularization**.\n",
        "  * For **multiclass classification** (works better for multiple classes).\n",
        "  * These solvers are faster and more efficient for **large-scale data**.\n",
        "\n",
        "* **Use `saga`**:\n",
        "\n",
        "  * When you need to support **both L1 and L2 regularization**.\n",
        "  * When you are working with **sparse datasets** or **large datasets**.\n",
        "  * If you are dealing with **multiclass classification** and need better flexibility.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary**\n",
        "\n",
        "* **liblinear**: Best for small datasets and **L1 regularization**.\n",
        "* **newton-cg**: Suitable for **large datasets** and **L2 regularization**.\n",
        "* **lbfgs**: Efficient for **large datasets** and **L2 regularization**, especially in **multiclass classification**.\n",
        "* **saga**: Great for **large datasets**, **sparse data**, and both **L1** and **L2 regularization**.\n",
        "\n"
      ],
      "metadata": {
        "id": "ywxff6LTC8Bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How is Logistic Regression extended for multiclass classification.\n",
        "- ### üîπ **Extending Logistic Regression for Multiclass Classification**\n",
        "\n",
        "In **Logistic Regression**, the basic model is designed for **binary classification** (i.e., predicting one of two classes). However, for **multiclass classification** (predicting more than two classes), logistic regression can be extended using two main approaches:\n",
        "\n",
        "1. **One-vs-Rest (OvR)**, also known as **One-vs-All (OvA)**\n",
        "2. **Multinomial (Softmax) Logistic Regression**\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. One-vs-Rest (OvR) / One-vs-All (OvA)**\n",
        "\n",
        "#### **Concept**:\n",
        "\n",
        "In **One-vs-Rest (OvR)**, we break down a **multiclass problem** into several **binary classification problems**. For each class, we train a separate binary logistic regression model that tries to distinguish between that class (the \"positive\" class) and all the other classes (the \"negative\" class).\n",
        "\n",
        "#### **How It Works**:\n",
        "\n",
        "* If there are **K classes**, we create **K binary classifiers**.\n",
        "* Each classifier predicts whether an instance belongs to the class it is trained for or not.\n",
        "* During prediction, each classifier outputs a probability, and the class with the highest probability is selected as the final prediction.\n",
        "\n",
        "#### **Example**:\n",
        "\n",
        "For a multiclass classification problem with 3 classes (`A`, `B`, and `C`):\n",
        "\n",
        "* The first classifier will distinguish between **A vs. not A** (B and C).\n",
        "* The second classifier will distinguish between **B vs. not B** (A and C).\n",
        "* The third classifier will distinguish between **C vs. not C** (A and B).\n",
        "\n",
        "During prediction:\n",
        "\n",
        "* If the first classifier outputs a probability of 0.7 for class **A**, the second outputs 0.2 for class **B**, and the third outputs 0.5 for class **C**,\n",
        "* The final prediction will be class **A** (since 0.7 is the highest probability).\n",
        "\n",
        "#### **Advantages**:\n",
        "\n",
        "* **Simple and easy to implement**.\n",
        "* Can be used with binary logistic regression as the base model.\n",
        "* Works well when classes are **well-separated**.\n",
        "\n",
        "#### **Disadvantages**:\n",
        "\n",
        "* **Not ideal for highly imbalanced classes**, as the classifier for the majority class will dominate the model.\n",
        "* Doesn't take into account the **interrelationships** between classes.\n",
        "\n",
        "#### **Implementation in `scikit-learn`**:\n",
        "\n",
        "`scikit-learn` uses **One-vs-Rest (OvR)** by default for multiclass classification in logistic regression. You can simply use the `LogisticRegression` class, and the library will automatically handle the OvR strategy.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create a logistic regression model for multiclass classification\n",
        "model = LogisticRegression(multi_class='ovr', solver='liblinear')\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "* The `multi_class='ovr'` argument specifies the **One-vs-Rest** strategy.\n",
        "* The solver can be chosen depending on the size of the dataset (`liblinear` is often used for small to medium datasets).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Multinomial (Softmax) Logistic Regression**\n",
        "\n",
        "#### **Concept**:\n",
        "\n",
        "**Multinomial Logistic Regression** is a generalization of binary logistic regression that directly models **multiclass classification**. It does so by using the **softmax function**, which is an extension of the logistic (sigmoid) function used in binary classification.\n",
        "\n",
        "#### **How It Works**:\n",
        "\n",
        "* The **softmax function** calculates the probability for each class as the **exponential of the linear scores** divided by the sum of the exponentials of all the scores:\n",
        "\n",
        "  $$\n",
        "  P(y = k | x) = \\frac{e^{\\theta_k^T x}}{\\sum_{j=1}^{K} e^{\\theta_j^T x}}\n",
        "  $$\n",
        "\n",
        "  Where:\n",
        "\n",
        "  * $P(y = k | x)$ is the probability of the instance belonging to class $k$,\n",
        "  * $\\theta_k^T x$ is the dot product of the feature vector $x$ and the parameter vector $\\theta_k$ for class $k$,\n",
        "  * $K$ is the total number of classes.\n",
        "\n",
        "#### **How It Differs from OvR**:\n",
        "\n",
        "* Instead of training multiple binary classifiers, multinomial logistic regression trains a **single model** that predicts the probability for each class simultaneously.\n",
        "* It is **more efficient** than the OvR approach since it directly handles all classes in a single model and accounts for the **interdependence** between them.\n",
        "\n",
        "#### **Advantages**:\n",
        "\n",
        "* **Accounts for interrelationships** between the classes.\n",
        "* **More efficient** for multiclass problems with many classes, as it requires fewer computations than training multiple binary classifiers.\n",
        "\n",
        "#### **Disadvantages**:\n",
        "\n",
        "* More computationally intensive, especially for a large number of classes.\n",
        "* May require more data for good performance on complex problems.\n",
        "\n",
        "#### **Implementation in `scikit-learn`**:\n",
        "\n",
        "In `scikit-learn`, **multinomial logistic regression** can be used by setting the `multi_class='multinomial'` argument. You also need to use an appropriate solver such as `lbfgs` or `saga` (since `liblinear` only supports **One-vs-Rest**).\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Create a multinomial logistic regression model\n",
        "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
        "\n",
        "# Fit the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "```\n",
        "\n",
        "* The `multi_class='multinomial'` argument specifies the **multinomial** approach.\n",
        "* The `solver='lbfgs'` is preferred because it works well with multinomial logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **When to Use One-vs-Rest vs. Multinomial Logistic Regression**\n",
        "\n",
        "| Feature                   | One-vs-Rest (OvR)                            | Multinomial Logistic Regression                     |\n",
        "| ------------------------- | -------------------------------------------- | --------------------------------------------------- |\n",
        "| **Number of Models**      | K binary classifiers for K classes           | One model for all classes                           |\n",
        "| **Solver**                | Supports `liblinear` and other solvers       | Requires `lbfgs` or `saga`                          |\n",
        "| **Interclass Dependency** | Doesn't account for class interdependence    | Accounts for interdependencies between classes      |\n",
        "| **Efficiency**            | Simple but requires training multiple models | More efficient with fewer models for large datasets |\n",
        "| **Data Size**             | Can be used for small datasets               | Preferred for large datasets and more classes       |\n",
        "\n",
        "* **Use One-vs-Rest (OvR)** when you need simplicity or when working with a small dataset and a binary classifier.\n",
        "* **Use Multinomial Logistic Regression** when you want a more **efficient model** that accounts for class interrelationships and is suitable for larger datasets.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary**\n",
        "\n",
        "* **One-vs-Rest (OvR)**: Breaks the multiclass problem into multiple binary problems. Simpler but less efficient for large datasets with many classes.\n",
        "* **Multinomial Logistic Regression (Softmax)**: Directly models the probabilities for each class simultaneously using the softmax function. More efficient and accounts for class dependencies, especially for large datasets with many classes.\n",
        "\n"
      ],
      "metadata": {
        "id": "8THtIV_oC7-B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are the advantages and disadvantages of Logistic Regression.\n",
        "- ### üîπ **Advantages and Disadvantages of Logistic Regression**\n",
        "\n",
        "**Logistic Regression** is a popular and widely used method for classification tasks. It is often chosen for its simplicity and interpretability, but like any model, it has its strengths and weaknesses. Below are the **advantages** and **disadvantages** of using logistic regression.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Advantages of Logistic Regression**\n",
        "\n",
        "#### 1. **Simplicity and Interpretability**\n",
        "\n",
        "* **Easy to understand**: Logistic regression is relatively straightforward and interpretable. The relationship between the independent variables and the dependent variable can be easily explained, making it an excellent choice when model transparency is important.\n",
        "* **Coefficients Interpretation**: The coefficients (weights) of the model indicate how each feature influences the probability of the outcome. For binary classification, the coefficients represent log-odds changes.\n",
        "\n",
        "#### 2. **Efficiency**\n",
        "\n",
        "* **Computationally efficient**: Logistic regression is relatively fast to train, especially for small to medium-sized datasets. This is because it requires less computation compared to more complex models like decision trees or neural networks.\n",
        "* **Fewer resources required**: Since logistic regression does not require extensive training time or memory, it is well-suited for environments with limited computational resources.\n",
        "\n",
        "#### 3. **Works Well with Linearly Separable Data**\n",
        "\n",
        "* **Good performance with linear boundaries**: Logistic regression works particularly well when the data is linearly separable or nearly so. In cases where the classes can be separated by a straight line (or hyperplane in higher dimensions), logistic regression tends to perform well.\n",
        "\n",
        "#### 4. **Probabilistic Interpretation**\n",
        "\n",
        "* **Probability output**: Logistic regression provides probabilities as output (values between 0 and 1), which can be useful for decision-making. This allows for more flexibility, especially when making threshold-based decisions.\n",
        "* **Can estimate confidence**: The probability outputs also provide insight into how confident the model is in its predictions, allowing users to adjust the decision threshold if needed.\n",
        "\n",
        "#### 5. **Works Well with High Dimensionality (Regularized Logistic Regression)**\n",
        "\n",
        "* **Effective with many features**: With regularization techniques (e.g., **L1** or **L2**), logistic regression can handle high-dimensional data and perform feature selection by shrinking irrelevant features‚Äô weights to zero (in the case of **L1 regularization**).\n",
        "\n",
        "#### 6. **Multiclass Classification**\n",
        "\n",
        "* **Can be extended to multiclass problems**: Logistic regression can be extended to multiclass classification problems using strategies like **One-vs-Rest (OvR)** or **multinomial logistic regression**, allowing it to handle more than two classes.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ùå **Disadvantages of Logistic Regression**\n",
        "\n",
        "#### 1. **Assumes Linear Decision Boundaries**\n",
        "\n",
        "* **Limited flexibility**: Logistic regression assumes a linear relationship between the independent variables and the log-odds of the dependent variable. This makes it less suitable for problems where the decision boundary is non-linear (e.g., XOR problem, complex datasets with intricate patterns).\n",
        "* If the data is **non-linearly separable**, logistic regression may perform poorly unless the features are transformed (e.g., using polynomial features, kernel trick).\n",
        "\n",
        "#### 2. **Sensitive to Outliers**\n",
        "\n",
        "* **Outlier sensitivity**: Logistic regression is sensitive to **outliers** because it tries to minimize the log-loss (which includes all data points in the calculation). A few extreme outliers can significantly affect the model's parameters and predictions.\n",
        "\n",
        "#### 3. **Requires a Large Amount of Data**\n",
        "\n",
        "* **Data requirements**: For logistic regression to perform well, you need a reasonable amount of data to accurately estimate the model parameters. With **small datasets**, the model may overfit or underfit the data.\n",
        "\n",
        "#### 4. **Multicollinearity Issues**\n",
        "\n",
        "* **Correlated features**: Logistic regression assumes that the features are independent. If there is **multicollinearity** (i.e., high correlation between independent variables), it can cause instability in the coefficient estimates and make the model sensitive to small changes in the data.\n",
        "* **Solution**: You can address this issue by applying techniques like **Principal Component Analysis (PCA)** or **removing correlated features**.\n",
        "\n",
        "#### 5. **Not Well-Suited for Complex Relationships**\n",
        "\n",
        "* **Limited ability to model complex patterns**: Logistic regression cannot capture complex relationships between features and the target variable, especially when there are interactions between features that influence the outcome in a non-linear way.\n",
        "* In cases where there are complex, high-order interactions or non-linear relationships, models like **decision trees**, **random forests**, or **neural networks** might be more appropriate.\n",
        "\n",
        "#### 6. **Assumption of Independence of Features**\n",
        "\n",
        "* **Feature independence assumption**: Logistic regression assumes that features are independent of each other, which is often not the case in real-world data. If the features are correlated, this assumption may be violated, leading to inaccurate predictions.\n",
        "\n",
        "#### 7. **Overfitting (without Regularization)**\n",
        "\n",
        "* **Risk of overfitting**: Logistic regression can easily overfit the data if the model is too complex (e.g., using many features) or if there is insufficient data. However, this issue can be mitigated by using **regularization** techniques like **L1** (Lasso) or **L2** (Ridge).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **When to Use Logistic Regression**\n",
        "\n",
        "Logistic Regression is best suited for:\n",
        "\n",
        "* Problems where there is a **clear linear relationship** between the features and the log-odds of the target variable.\n",
        "* **Binary classification tasks** or **multiclass classification** (using One-vs-Rest or multinomial extensions).\n",
        "* Situations where you need **probability estimates** along with the classification outcome.\n",
        "* When the data is relatively **clean** and does not contain significant outliers or multicollinearity.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary**\n",
        "\n",
        "| **Advantages**                                                             | **Disadvantages**                                                                   |\n",
        "| -------------------------------------------------------------------------- | ----------------------------------------------------------------------------------- |\n",
        "| **Simplicity and interpretability**: Easy to understand and explain.       | **Assumes linear decision boundaries**: Performs poorly with complex relationships. |\n",
        "| **Computationally efficient**: Fast and requires less computational power. | **Sensitive to outliers**: Outliers can affect the model's performance.             |\n",
        "| **Probabilistic output**: Provides probabilities for classification.       | **Requires large datasets**: Can perform poorly with small datasets.                |\n",
        "| **Works well with linearly separable data**.                               | **Multicollinearity**: Issues with correlated features.                             |\n",
        "| **Can handle high-dimensional data with regularization**.                  | **Not suitable for complex relationships**: Struggles with non-linear patterns.     |\n",
        "| **Can be extended to multiclass problems**.                                | **Overfitting risk**: Can overfit without regularization.                           |\n",
        "\n",
        "While logistic regression is simple, fast, and interpretable, it is not suitable for all situations. It works best when the data has a **linear relationship** and doesn't contain complex interactions or non-linear patterns. For more complicated data, more sophisticated models like **decision trees**, **random forests**, or **neural networks** may be more appropriate.\n",
        "\n"
      ],
      "metadata": {
        "id": "Tlj9a_VJC76W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are some use cases of Logistic Regression.\n",
        "- ### üîπ **Use Cases of Logistic Regression**\n",
        "\n",
        "Logistic Regression is widely used in various domains due to its simplicity, efficiency, and interpretability. Below are some common use cases where Logistic Regression excels:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. Binary Classification Tasks**\n",
        "\n",
        "Since logistic regression is primarily designed for binary classification, it is frequently used in applications where the goal is to predict one of two possible outcomes.\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "* **Spam Detection**:\n",
        "\n",
        "  * Predict whether an email is **spam** or **not spam**.\n",
        "  * **Input**: Features could include words, frequency of certain terms, email metadata, etc.\n",
        "* **Fraud Detection**:\n",
        "\n",
        "  * Classify financial transactions as **fraudulent** or **non-fraudulent**.\n",
        "  * **Input**: Features like transaction amount, location, time of day, account history, etc.\n",
        "* **Customer Churn Prediction**:\n",
        "\n",
        "  * Predict if a customer will **leave** a service provider (e.g., telecom, subscription service) or not.\n",
        "  * **Input**: Customer activity, service usage patterns, account information, etc.\n",
        "* **Medical Diagnoses**:\n",
        "\n",
        "  * Predict the **presence** or **absence** of a disease (e.g., **cancer** detection, **heart disease** prediction).\n",
        "  * **Input**: Features could be patient demographics, medical history, test results, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Multiclass Classification**\n",
        "\n",
        "Logistic regression can also be extended to handle multiple classes using **One-vs-Rest (OvR)** or **Multinomial Logistic Regression**. This is useful in situations where there are more than two possible categories.\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "* **Digit Recognition**:\n",
        "\n",
        "  * Classify handwritten digits (0‚Äì9).\n",
        "  * **Input**: Image pixel values.\n",
        "* **Sentiment Analysis**:\n",
        "\n",
        "  * Classify the sentiment of a piece of text as **positive**, **negative**, or **neutral**.\n",
        "  * **Input**: Text features like word counts or TF-IDF scores.\n",
        "* **Customer Satisfaction**:\n",
        "\n",
        "  * Predict customer satisfaction on a scale (e.g., **very dissatisfied**, **dissatisfied**, **neutral**, **satisfied**, **very satisfied**).\n",
        "  * **Input**: Customer feedback, survey responses, etc.\n",
        "* **Image Classification**:\n",
        "\n",
        "  * Classify images into different categories (e.g., **dog**, **cat**, **bird**, etc.).\n",
        "  * **Input**: Image features like pixel values, color histograms, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. Probability Estimation**\n",
        "\n",
        "Logistic regression is widely used for estimating the probability of an event occurring, which is useful in many real-world applications.\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "* **Risk Assessment**:\n",
        "\n",
        "  * Estimate the probability that a **patient** will develop a certain condition (e.g., likelihood of heart attack within the next 10 years).\n",
        "  * **Input**: Risk factors like age, cholesterol levels, blood pressure, smoking status, etc.\n",
        "* **Loan Approval**:\n",
        "\n",
        "  * Predict the probability that a **loan applicant** will default on a loan.\n",
        "  * **Input**: Features like income, credit score, previous loan history, etc.\n",
        "* **Marketing Campaign Effectiveness**:\n",
        "\n",
        "  * Estimate the probability of a **customer** responding to a marketing campaign (e.g., clicking an ad or making a purchase).\n",
        "  * **Input**: Features like past purchase behavior, demographics, and interaction history.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **4. A/B Testing and Conversion Optimization**\n",
        "\n",
        "Logistic regression can be used to model the probability of a conversion event occurring as part of **A/B testing** or **conversion optimization** experiments.\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "* **Website Optimization**:\n",
        "\n",
        "  * Predict the likelihood that a visitor will **convert** (e.g., make a purchase or sign up) based on the variant of a webpage they see.\n",
        "  * **Input**: Features like user demographics, page design variant, time spent on site, etc.\n",
        "* **Ad Click-Through Rate (CTR) Prediction**:\n",
        "\n",
        "  * Predict the probability that a user will **click** on an ad given the ad's content and user‚Äôs interaction history.\n",
        "  * **Input**: Features like ad text, placement, user‚Äôs browsing history, and demographics.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **5. Credit Scoring and Financial Applications**\n",
        "\n",
        "Logistic regression is used extensively in **credit scoring** and other financial modeling tasks where a binary classification or probability is needed.\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "* **Credit Scoring**:\n",
        "\n",
        "  * Predict whether a **loan applicant** is **creditworthy** (i.e., will default or not).\n",
        "  * **Input**: Features like credit history, income, employment status, debt-to-income ratio, etc.\n",
        "* **Insurance Underwriting**:\n",
        "\n",
        "  * Predict the probability that a customer will file an **insurance claim**.\n",
        "  * **Input**: Features like age, health status, type of coverage, etc.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **6. Marketing and Customer Segmentation**\n",
        "\n",
        "Logistic regression is also useful in **segmentation tasks** where the goal is to categorize customers into distinct groups.\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "* **Targeting High-Value Customers**:\n",
        "\n",
        "  * Classify customers as **high-value** or **low-value** for targeted marketing efforts.\n",
        "  * **Input**: Features like past spending behavior, interaction history, and demographics.\n",
        "* **Personalized Product Recommendations**:\n",
        "\n",
        "  * Predict whether a customer will be interested in a specific **product** or **service**.\n",
        "  * **Input**: Features such as past purchase data, browsing behavior, and preferences.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **7. Predictive Maintenance**\n",
        "\n",
        "Logistic regression can be used to predict the likelihood of a **machine** or **equipment** failure in industrial or manufacturing settings, helping to schedule timely maintenance.\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "* **Machine Failure Prediction**:\n",
        "\n",
        "  * Predict the likelihood that a machine will fail based on usage data, maintenance history, and sensor readings.\n",
        "  * **Input**: Features like operating hours, temperature, pressure, and vibration levels.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **8. Healthcare Applications**\n",
        "\n",
        "Logistic regression is used in healthcare for diagnostic prediction, patient monitoring, and treatment planning.\n",
        "\n",
        "#### **Examples:**\n",
        "\n",
        "* **Disease Risk Prediction**:\n",
        "\n",
        "  * Predict the probability of a **patient** developing a disease (e.g., diabetes, stroke, or cancer).\n",
        "  * **Input**: Features like lifestyle factors, family history, and clinical measurements.\n",
        "* **Treatment Effectiveness**:\n",
        "\n",
        "  * Predict whether a patient will respond to a specific treatment or drug.\n",
        "  * **Input**: Patient's medical history, treatment type, age, and other clinical factors.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary of Use Cases**\n",
        "\n",
        "| **Use Case**                              | **Description**                                                    | **Example**                                                           |\n",
        "| ----------------------------------------- | ------------------------------------------------------------------ | --------------------------------------------------------------------- |\n",
        "| **Binary Classification**                 | Predicting one of two outcomes.                                    | Email spam detection, fraud detection, churn prediction.              |\n",
        "| **Multiclass Classification**             | Predicting one of multiple classes.                                | Digit recognition, sentiment analysis, image classification.          |\n",
        "| **Probability Estimation**                | Estimating the probability of an event.                            | Risk assessment, loan approval, marketing campaign effectiveness.     |\n",
        "| **A/B Testing & Conversion Optimization** | Estimating likelihood of conversions or clicks.                    | Website optimization, ad click-through rate prediction.               |\n",
        "| **Credit Scoring**                        | Predicting creditworthiness or financial behavior.                 | Loan approval, insurance underwriting.                                |\n",
        "| **Customer Segmentation**                 | Classifying customers into distinct groups for marketing purposes. | Targeting high-value customers, personalized product recommendations. |\n",
        "| **Predictive Maintenance**                | Predicting when equipment or machinery will fail.                  | Machine failure prediction, equipment breakdown.                      |\n",
        "| **Healthcare**                            | Predicting disease or treatment outcomes in medical contexts.      | Disease risk prediction, treatment effectiveness.                     |\n",
        "\n",
        "---\n",
        "\n",
        "Logistic regression‚Äôs **simplicity** and ability to produce **probabilistic outputs** make it a useful tool in many domains, particularly when a clear decision boundary or risk score is needed.\n",
        "\n"
      ],
      "metadata": {
        "id": "Z45wLsdMBzHo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What is the difference between Softmax Regression and Logistic Regression.\n",
        "- ### üîπ **Difference Between Softmax Regression and Logistic Regression**\n",
        "\n",
        "Both **Logistic Regression** and **Softmax Regression** (also known as **Multinomial Logistic Regression**) are algorithms used for classification tasks, but they are applied to different types of classification problems. The main difference lies in the type of classification they are suited for: **binary classification** versus **multiclass classification**. Let‚Äôs break down the key differences:\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. Type of Classification**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * **Binary Classification**: Logistic regression is used for **binary classification**, where the output variable has only **two possible classes** (e.g., **yes/no**, **spam/ham**, **0/1**).\n",
        "  * The model predicts the probability of an instance belonging to the **positive class** (class 1) versus the **negative class** (class 0).\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * **Multiclass Classification**: Softmax regression is used for **multiclass classification**, where the output variable can have **more than two classes** (e.g., **cat**, **dog**, **bird**, etc.).\n",
        "  * Softmax regression generalizes logistic regression to handle multiple classes simultaneously, assigning a probability distribution over **K** classes, where the sum of probabilities equals 1.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Output**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Logistic regression outputs a single probability, which represents the likelihood that the instance belongs to a certain class (usually the **positive class**).\n",
        "  * The output is a value between **0** and **1**, which is interpreted as the probability of the instance belonging to the **positive class**.\n",
        "  * The decision rule: If the probability is greater than 0.5, the instance is classified as **class 1**; otherwise, it is classified as **class 0**.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Softmax regression outputs **K probabilities**, one for each class in a **multiclass** problem.\n",
        "  * The probabilities are calculated using the **softmax function**, which converts the raw scores (logits) into probabilities. These probabilities sum up to **1**.\n",
        "  * The decision rule: The class with the **highest probability** is chosen as the predicted class.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. Function Used**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * The output of logistic regression is calculated using the **sigmoid function** (also known as the logistic function). The sigmoid function maps any real-valued number into a range between **0** and **1**.\n",
        "\n",
        "  $$\n",
        "  P(y=1|X) = \\frac{1}{1 + e^{-\\theta^T X}}\n",
        "  $$\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Softmax regression uses the **softmax function**, which is an extension of the logistic function. The softmax function is used to compute a probability distribution over **K** possible classes.\n",
        "\n",
        "  $$\n",
        "  P(y=k|X) = \\frac{e^{\\theta_k^T X}}{\\sum_{j=1}^{K} e^{\\theta_j^T X}}\n",
        "  $$\n",
        "\n",
        "  Where:\n",
        "\n",
        "  * $\\theta_k^T X$ is the raw score (logit) for class $k$,\n",
        "  * $K$ is the total number of classes,\n",
        "  * The denominator is the sum of exponentials of all logits, ensuring that the output probabilities sum to 1.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **4. Cost Function**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Logistic regression uses the **binary cross-entropy (log loss)** as the cost function for binary classification. The goal is to minimize the negative log-likelihood of the predicted probabilities.\n",
        "\n",
        "  $$\n",
        "  J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right)\n",
        "  $$\n",
        "\n",
        "  Where:\n",
        "\n",
        "  * $h_{\\theta}(x)$ is the logistic regression model‚Äôs predicted probability.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Softmax regression uses the **categorical cross-entropy** (also known as **multiclass log loss**) as the cost function. This cost function is designed for **multiclass classification** problems.\n",
        "\n",
        "  $$\n",
        "  J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(P(y_k^{(i)}|x^{(i)}))\n",
        "  $$\n",
        "\n",
        "  Where:\n",
        "\n",
        "  * $P(y_k^{(i)}|x^{(i)})$ is the predicted probability for class $k$ for the $i$-th sample,\n",
        "  * $y_k^{(i)}$ is the **one-hot encoded** label for class $k$ of sample $i$.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **5. Application**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Typically used for **binary classification** tasks such as:\n",
        "\n",
        "    * Email spam detection (**spam or not spam**),\n",
        "    * Disease detection (**sick or healthy**),\n",
        "    * Customer churn prediction (**churn or no churn**), etc.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Typically used for **multiclass classification** tasks such as:\n",
        "\n",
        "    * Image classification (**dog, cat, bird, etc.**),\n",
        "    * Sentiment analysis (**positive, neutral, negative**),\n",
        "    * Handwritten digit recognition (**0 to 9**), etc.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **6. Decision Boundaries**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * In logistic regression, the decision boundary is a **line** (in 2D) or **hyperplane** (in higher dimensions) that separates the two classes.\n",
        "  * The decision boundary is defined by the **logistic function** where the probability of belonging to class 1 is 0.5.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * In softmax regression, the decision boundary is more **complex** because it involves multiple classes.\n",
        "  * The decision boundary is determined by the **highest probability** among all classes, leading to a set of boundaries that separate the classes in the feature space.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **7. Solver**\n",
        "\n",
        "* **Logistic Regression**:\n",
        "\n",
        "  * Logistic regression can be solved using optimization techniques such as **gradient descent**, **stochastic gradient descent (SGD)**, or **Newton‚Äôs method**.\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * Softmax regression also typically uses **gradient descent** or **SGD**, but it must optimize for multiple classes simultaneously.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary of Key Differences**\n",
        "\n",
        "| **Feature**                | **Logistic Regression**                             | **Softmax Regression**                                      |\n",
        "| -------------------------- | --------------------------------------------------- | ----------------------------------------------------------- |\n",
        "| **Type of Classification** | Binary classification (two classes)                 | Multiclass classification (more than two classes)           |\n",
        "| **Output**                 | Single probability (for one class)                  | Probability distribution over all classes                   |\n",
        "| **Function Used**          | Sigmoid function                                    | Softmax function                                            |\n",
        "| **Cost Function**          | Binary cross-entropy (log loss)                     | Categorical cross-entropy (multiclass log loss)             |\n",
        "| **Decision Boundary**      | Linear decision boundary (between two classes)      | Multiple boundaries for multiple classes                    |\n",
        "| **Common Use Cases**       | Spam detection, fraud detection, disease prediction | Image classification, sentiment analysis, digit recognition |\n",
        "| **Solver**                 | Gradient descent, Newton's method                   | Gradient descent, SGD                                       |\n",
        "\n",
        "---\n",
        "\n",
        "### üîÑ **In Conclusion**:\n",
        "\n",
        "* **Logistic Regression** is best for **binary classification** tasks, predicting the probability of one class (usually the \"positive\" class).\n",
        "* **Softmax Regression** is an extension of logistic regression that can handle **multiclass classification** by assigning probabilities to multiple classes and choosing the class with the highest probability.\n",
        "\n"
      ],
      "metadata": {
        "id": "0fwkHBCaFYHT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification.\n",
        "- ### üîπ **Choosing Between One-vs-Rest (OvR) and Softmax for Multiclass Classification**\n",
        "\n",
        "When working with **multiclass classification**, the two most common approaches are **One-vs-Rest (OvR)** and **Softmax Regression** (Multinomial Logistic Regression). Both have their advantages, and the choice between the two often depends on the specific problem, the dataset, and the model's goals. Let's break down the key differences, advantages, and considerations when choosing between these two methods.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. One-vs-Rest (OvR) Classification**\n",
        "\n",
        "In **One-vs-Rest**, the idea is to train multiple **binary classifiers** (one for each class). Each classifier predicts whether the instance belongs to its class (**positive** class) or not (**negative** class).\n",
        "\n",
        "#### **How It Works:**\n",
        "\n",
        "* For a **K-class** problem, you train **K binary classifiers**, where each classifier is trained to distinguish between one class and the others. For example:\n",
        "\n",
        "  * Classifier 1: Predict class 1 vs. all others.\n",
        "  * Classifier 2: Predict class 2 vs. all others.\n",
        "  * ...\n",
        "  * Classifier K: Predict class K vs. all others.\n",
        "* When making a prediction, the classifier with the **highest confidence score** (or probability) is selected as the final prediction.\n",
        "\n",
        "#### **Advantages of OvR**:\n",
        "\n",
        "1. **Simplicity**: Each classifier is a standard binary classifier, so you can apply well-established algorithms (like logistic regression, SVM, etc.) for each binary classification.\n",
        "2. **Flexibility**: Works with any binary classifier, including logistic regression, decision trees, and others. It doesn't require a specific algorithm.\n",
        "3. **Scalability**: The model can be trained in parallel for each class, making it efficient for large datasets and when individual classifiers are relatively simple.\n",
        "\n",
        "#### **Disadvantages of OvR**:\n",
        "\n",
        "1. **Multiple classifiers**: You need to train **K classifiers** (one for each class), which can lead to increased memory usage and computation, especially for large values of **K**.\n",
        "2. **Class imbalance**: The classifier may perform poorly if the classes are highly imbalanced since it treats each class separately and doesn‚Äôt consider the global context.\n",
        "3. **Inconsistent predictions**: The classifiers may not always provide consistent predictions (i.e., more than one classifier can predict \"positive\" for the same instance).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Softmax Regression (Multinomial Logistic Regression)**\n",
        "\n",
        "**Softmax Regression** (also known as **Multinomial Logistic Regression**) is a direct extension of logistic regression for multiclass problems. It predicts a **probability distribution** over all classes and assigns the class with the highest probability.\n",
        "\n",
        "#### **How It Works:**\n",
        "\n",
        "* The model learns **K-1** parameters for each class (for **K** classes). It uses the **softmax function** to convert the raw scores (logits) into probabilities.\n",
        "\n",
        "* The softmax function calculates the probability for each class:\n",
        "\n",
        "  $$\n",
        "  P(y=k | X) = \\frac{e^{\\theta_k^T X}}{\\sum_{j=1}^{K} e^{\\theta_j^T X}}\n",
        "  $$\n",
        "\n",
        "* After computing the probabilities for each class, the class with the highest probability is selected as the final prediction.\n",
        "\n",
        "#### **Advantages of Softmax Regression**:\n",
        "\n",
        "1. **Direct approach**: Softmax regression directly optimizes for the multiclass problem, making it more natural and effective for problems with multiple classes.\n",
        "2. **Global decision**: Since softmax considers all classes simultaneously, it can account for the relationships between the classes, making the decision more **consistent** across all classes.\n",
        "3. **Probabilistic output**: Softmax regression provides a **probability distribution** across all classes, which can be useful in many applications, such as determining the **uncertainty** of predictions.\n",
        "\n",
        "#### **Disadvantages of Softmax Regression**:\n",
        "\n",
        "1. **Single classifier**: Softmax regression is a single model that directly learns to classify all classes, which can sometimes be more complex than the multiple binary classifiers in OvR.\n",
        "2. **Computational cost**: For **K** classes, softmax regression computes the probability for each class simultaneously, so it can be more computationally expensive when **K** is large.\n",
        "3. **Model complexity**: Softmax regression assumes that the classes are **mutually exclusive**, which might not hold true in all real-world scenarios.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **When to Choose One-vs-Rest (OvR) vs. Softmax Regression**\n",
        "\n",
        "| **Criteria**                            | **One-vs-Rest (OvR)**                                                                                                             | **Softmax Regression**                                                                              |\n",
        "| --------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------- |\n",
        "| **Number of Classes (K)**               | Works well for both small and large values of **K**.                                                                              | Computationally expensive for very large **K**.                                                     |\n",
        "| **Computational Complexity**            | Requires training **K** classifiers (parallelizable).                                                                             | Requires training a single classifier with **K** classes.                                           |\n",
        "| **Interpretability**                    | Each classifier is a simple binary classifier, which can be easy to interpret.                                                    | Single model that directly classifies all classes.                                                  |\n",
        "| **Class Imbalance**                     | Can perform poorly with imbalanced classes for some classifiers.                                                                  | Can also suffer from imbalanced classes, but softmax handles it more robustly.                      |\n",
        "| **Consistency of Predictions**          | Can have inconsistent predictions (more than one classifier may predict a positive class).                                        | More consistent as the probabilities sum to 1 across all classes.                                   |\n",
        "| **Performance on Multi-class Problems** | Can be less efficient and less accurate for large **K**.                                                                          | More suitable for large-scale **K**-class classification problems.                                  |\n",
        "| **Use Case**                            | Suitable when any binary classifier (like logistic regression) is used, especially when computational cost needs to be minimized. | Best when you need a direct **multiclass classification** approach with a probability distribution. |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **When to Use One-vs-Rest (OvR):**\n",
        "\n",
        "* **Large number of classes (K)**: When the number of classes is very large, **OvR** can be more efficient as it trains **K** separate classifiers, which can be parallelized.\n",
        "* **Simple classifiers**: If you're using simpler classifiers (e.g., logistic regression, support vector machines), **OvR** is a good choice.\n",
        "* **Computational efficiency**: If training time is a concern, and you're using simpler classifiers, OvR might be faster.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **When to Use Softmax Regression:**\n",
        "\n",
        "* **Small to medium-sized K**: When the number of classes is not too large, **softmax regression** is typically more efficient as it avoids the overhead of training multiple classifiers.\n",
        "* **Global class relationships matter**: If the classes have dependencies or interactions, **softmax regression** is the better choice because it models the classes simultaneously and considers the relative probabilities.\n",
        "* **Probabilistic output**: If you need **probability distributions** over the classes or you need to model the **uncertainty** of predictions, softmax regression provides that output.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary**\n",
        "\n",
        "* **OvR (One-vs-Rest)**:\n",
        "\n",
        "  * **Better for large K** or when you need a simple, parallelizable approach.\n",
        "  * It involves training multiple binary classifiers and can be used with any binary classifier (e.g., logistic regression, SVM).\n",
        "  * **Pros**: Flexibility, scalability, works well with simple classifiers.\n",
        "  * **Cons**: May lead to inconsistent predictions, especially in the case of class imbalance.\n",
        "\n",
        "* **Softmax Regression**:\n",
        "\n",
        "  * **Better for direct multiclass classification** and when you need a **global view** of all classes.\n",
        "  * It uses a single classifier to model the entire classification problem and provides a **probability distribution** across classes.\n",
        "  * **Pros**: More consistent, provides a probabilistic output, better for complex relationships between classes.\n",
        "  * **Cons**: Can be computationally expensive with large **K** classes, may require more resources for large datasets.\n",
        "\n"
      ],
      "metadata": {
        "id": "pUF1bob9FZFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20.  How do we interpret coefficients in Logistic Regression?\n",
        "- ### üîπ **Interpreting Coefficients in Logistic Regression**\n",
        "\n",
        "In **logistic regression**, the model produces a set of **coefficients (weights)** that describe the relationship between the **input features** and the **log-odds** of the outcome. Interpreting these coefficients helps you understand how each feature influences the predicted probability of the outcome.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **1. Logistic Regression Model Recap**\n",
        "\n",
        "The logistic regression model is based on the following equation:\n",
        "\n",
        "$$\n",
        "\\log \\left( \\frac{P(y=1 | X)}{1 - P(y=1 | X)} \\right) = \\theta_0 + \\theta_1 X_1 + \\theta_2 X_2 + \\dots + \\theta_n X_n\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "* $P(y=1 | X)$ is the probability of the positive class (class 1) given the input features $X$.\n",
        "* $\\frac{P(y=1 | X)}{1 - P(y=1 | X)}$ is the **odds** of the event occurring.\n",
        "* $\\theta_0, \\theta_1, \\dots, \\theta_n$ are the model coefficients (weights).\n",
        "* $X_1, X_2, \\dots, X_n$ are the feature values.\n",
        "\n",
        "The **log-odds** (the left-hand side of the equation) is transformed into a probability using the **sigmoid function**:\n",
        "\n",
        "$$\n",
        "P(y=1 | X) = \\frac{1}{1 + e^{-(\\theta_0 + \\theta_1 X_1 + \\dots + \\theta_n X_n)}}\n",
        "$$\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Interpreting the Coefficients (Œ∏)**\n",
        "\n",
        "#### **Understanding the Coefficients**:\n",
        "\n",
        "Each coefficient $\\theta_i$ in the logistic regression model represents the **change in the log-odds** of the outcome for a **one-unit increase** in the corresponding feature $X_i$, holding all other features constant.\n",
        "\n",
        "#### **Sign of the Coefficients**:\n",
        "\n",
        "* **Positive coefficient ($\\theta_i > 0$)**:\n",
        "\n",
        "  * A positive coefficient indicates that as $X_i$ increases, the **odds of the event** occurring (class 1) **increase**. In other words, higher values of $X_i$ make it more likely for the outcome to be 1 (the positive class).\n",
        "* **Negative coefficient ($\\theta_i < 0$)**:\n",
        "\n",
        "  * A negative coefficient means that as $X_i$ increases, the **odds of the event** occurring (class 1) **decrease**. In other words, higher values of $X_i$ make it less likely for the outcome to be 1.\n",
        "\n",
        "#### **Magnitude of the Coefficients**:\n",
        "\n",
        "The **magnitude** (absolute value) of the coefficient $\\theta_i$ tells you how **strongly** the feature $X_i$ is associated with the outcome.\n",
        "\n",
        "* Larger values of $\\theta_i$ (in absolute terms) imply that the corresponding feature has a **stronger effect** on the probability of the event occurring.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. Interpreting Coefficients in Terms of Odds Ratios**\n",
        "\n",
        "The logistic regression coefficients can be interpreted as **log-odds**, but often, it is more intuitive to interpret them as **odds ratios**.\n",
        "\n",
        "* **Odds Ratio** = $e^{\\theta_i}$\n",
        "\n",
        "The **odds ratio** gives us the **multiplicative change in the odds** of the outcome for a **one-unit increase** in the corresponding feature $X_i$, holding all other features constant.\n",
        "\n",
        "#### **How to interpret the odds ratio**:\n",
        "\n",
        "* **If $\\theta_i > 0$** (positive coefficient):\n",
        "\n",
        "  * The odds ratio $e^{\\theta_i}$ will be **greater than 1**.\n",
        "  * This means that a one-unit increase in $X_i$ **increases** the odds of the outcome (class 1).\n",
        "  * For example, if $\\theta_i = 0.5$, the odds ratio is $e^{0.5} \\approx 1.65$, meaning the odds of the event occurring increase by 65% for every one-unit increase in $X_i$.\n",
        "\n",
        "* **If $\\theta_i < 0$** (negative coefficient):\n",
        "\n",
        "  * The odds ratio $e^{\\theta_i}$ will be **less than 1**.\n",
        "  * This means that a one-unit increase in $X_i$ **decreases** the odds of the outcome (class 1).\n",
        "  * For example, if $\\theta_i = -0.5$, the odds ratio is $e^{-0.5} \\approx 0.61$, meaning the odds of the event occurring decrease by 39% for every one-unit increase in $X_i$.\n",
        "\n",
        "* **If $\\theta_i = 0$**:\n",
        "\n",
        "  * The odds ratio is $e^0 = 1$, meaning that the feature $X_i$ has **no effect** on the odds of the outcome.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **4. Example: Interpreting Coefficients in a Logistic Regression Model**\n",
        "\n",
        "Consider a logistic regression model used for predicting whether a customer will buy a product (class 1) or not (class 0), with two features: **Age** and **Income**.\n",
        "\n",
        "Let's assume the following coefficients:\n",
        "\n",
        "$$\n",
        "\\log \\left( \\frac{P(y=1)}{1 - P(y=1)} \\right) = -2 + 0.03 \\times \\text{Age} + 0.01 \\times \\text{Income}\n",
        "$$\n",
        "\n",
        "* **Intercept**: $\\theta_0 = -2$\n",
        "\n",
        "  * The intercept of -2 suggests that when **Age = 0** and **Income = 0**, the log-odds of the customer buying the product is -2. This corresponds to a low initial probability of purchasing the product.\n",
        "\n",
        "* **Age coefficient**: $\\theta_1 = 0.03$\n",
        "\n",
        "  * For every **one-year increase** in age, the log-odds of the customer buying the product **increase** by 0.03.\n",
        "  * The **odds ratio** for age is $e^{0.03} \\approx 1.03$, meaning that for every additional year of age, the **odds of purchasing the product increase by 3%**.\n",
        "\n",
        "* **Income coefficient**: $\\theta_2 = 0.01$\n",
        "\n",
        "  * For every **one-unit increase** in income, the log-odds of purchasing the product **increase** by 0.01.\n",
        "  * The **odds ratio** for income is $e^{0.01} \\approx 1.01$, meaning that for every additional unit of income, the **odds of purchasing the product increase by 1%**.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **5. Statistical Significance of Coefficients**\n",
        "\n",
        "When interpreting logistic regression coefficients, it is also important to consider their **statistical significance**:\n",
        "\n",
        "* **P-values**: A low p-value (typically < 0.05) indicates that the corresponding coefficient is statistically significant, meaning that the feature has a meaningful relationship with the outcome.\n",
        "* **Confidence Intervals**: The confidence interval for each coefficient provides a range of values within which the true coefficient is likely to fall. If the interval contains 0, the feature might not have a significant effect.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **Summary: How to Interpret Coefficients in Logistic Regression**\n",
        "\n",
        "* **Positive coefficient**: As the feature increases, the odds of the positive class (class 1) increase.\n",
        "* **Negative coefficient**: As the feature increases, the odds of the positive class (class 1) decrease.\n",
        "* **Magnitude of coefficient**: Larger values of the coefficient (in absolute terms) indicate a stronger influence on the outcome.\n",
        "* **Odds Ratio**: $e^{\\theta_i}$ gives the multiplicative effect of a one-unit increase in the feature on the odds of the outcome.\n",
        "\n"
      ],
      "metadata": {
        "id": "h734gDERFZB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Practical"
      ],
      "metadata": {
        "id": "YMLJT7CnFY-X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "Regression, and prints the model accuracy.\n"
      ],
      "metadata": {
        "id": "bfLJRbcRFY6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train a Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict the test set results\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmC7CkT__oxZ",
        "outputId": "e387c7d0-00ab-47c0-a565-89d549ba1cb4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "and print the model accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "LpybuyuoGyIu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train a Logistic Regression model with L1 regularization (Lasso)\n",
        "log_reg_lasso = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
        "log_reg_lasso.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict the test set results\n",
        "y_pred = log_reg_lasso.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression (L1 Regularization - Lasso) Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gd0Ff9rc_osh",
        "outputId": "86a358e3-0aaf-4643-a536-6ea32c9e5e65"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (L1 Regularization - Lasso) Model Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "LogisticRegression(penalty='l2'). Print model accuracy and coefficients."
      ],
      "metadata": {
        "id": "kr4y3OrqHGiE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R2kt647S_fov",
        "outputId": "81b59cb3-1dcb-47a9-b057-e321c259e60b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (L2 Regularization - Ridge) Model Accuracy: 97.78%\n",
            "Model Coefficients:\n",
            "[[ 0.36479402  1.35499766 -2.09628559 -0.92154751]\n",
            " [ 0.4808915  -1.58463288  0.3937527  -1.09224057]\n",
            " [-1.5286415  -1.43244729  2.3048277   2.08584535]]\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train a Logistic Regression model with L2 regularization (Ridge)\n",
        "log_reg_ridge = LogisticRegression(penalty='l2', solver='liblinear', max_iter=200)\n",
        "log_reg_ridge.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict the test set results\n",
        "y_pred = log_reg_ridge.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression (L2 Regularization - Ridge) Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 6: Print the coefficients of the model\n",
        "print(\"Model Coefficients:\")\n",
        "print(log_reg_ridge.coef_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. #C Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')."
      ],
      "metadata": {
        "id": "ZHIJIwCzHROZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train a Logistic Regression model with Elastic Net regularization\n",
        "log_reg_elastic_net = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "log_reg_elastic_net.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict the test set results\n",
        "y_pred = log_reg_elastic_net.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression (Elastic Net Regularization) Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 6: Print the coefficients of the model\n",
        "print(\"Model Coefficients:\")\n",
        "print(log_reg_elastic_net.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-p7fLdYiHO88",
        "outputId": "0de00ce4-bfeb-4aa0-80df-6c8e6922f916"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (Elastic Net Regularization) Model Accuracy: 100.00%\n",
            "Model Coefficients:\n",
            "[[ 0.38851695  1.70786826 -2.35276673 -0.66069032]\n",
            " [ 0.0408462   0.          0.         -0.54057061]\n",
            " [-1.22070004 -1.28779926  2.46773459  1.9916528 ]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. C Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "multi_class='ovr'."
      ],
      "metadata": {
        "id": "-nF0ggoTHewT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Initialize and train a Logistic Regression model with One-vs-Rest (OvR) multiclass strategy\n",
        "log_reg_ovr = LogisticRegression(multi_class='ovr', solver='liblinear', max_iter=200)\n",
        "\n",
        "# Train the model\n",
        "log_reg_ovr.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Predict the test set results\n",
        "y_pred = log_reg_ovr.predict(X_test)\n",
        "\n",
        "# Step 5: Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Logistic Regression (One-vs-Rest Strategy) Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Step 6: Print the model's coefficients\n",
        "print(\"Model Coefficients:\")\n",
        "print(log_reg_ovr.coef_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KB7bwsEHchU",
        "outputId": "e0cd8633-67c2-4494-e898-cb7b29c3d1b0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic Regression (One-vs-Rest Strategy) Model Accuracy: 97.78%\n",
            "Model Coefficients:\n",
            "[[ 0.36479402  1.35499766 -2.09628559 -0.92154751]\n",
            " [ 0.4808915  -1.58463288  0.3937527  -1.09224057]\n",
            " [-1.5286415  -1.43244729  2.3048277   2.08584535]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "Regression. Print the best parameters and accuracy."
      ],
      "metadata": {
        "id": "zv1Eej4IH4vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Set up the Logistic Regression model and parameter grid for GridSearchCV\n",
        "log_reg = LogisticRegression(max_iter=200, multi_class='ovr')\n",
        "\n",
        "# Define the parameter grid for hyperparameter tuning\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],  # Regularization strength\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Regularization types\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support the respective penalties\n",
        "}\n",
        "\n",
        "# Step 4: Set up GridSearchCV with cross-validation\n",
        "grid_search = GridSearchCV(estimator=log_reg, param_grid=param_grid, cv=5, verbose=1, n_jobs=-1)\n",
        "\n",
        "# Step 5: Fit the GridSearchCV model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Print the best hyperparameters and the accuracy of the model\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Step 7: Evaluate the best model on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWWoA010HzdL",
        "outputId": "8eb684b0-2bb6-419b-ea2c-70d4b72b4af7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
            "Best Hyperparameters: {'C': 1, 'penalty': 'l1', 'solver': 'saga'}\n",
            "Best Model Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "30 fits failed out of a total of 90.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.67619048 0.84761905 0.8        0.87619048        nan        nan\n",
            " 0.94285714 0.95238095 0.94285714 0.95238095        nan        nan\n",
            " 0.94285714 0.94285714 0.95238095 0.94285714        nan        nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "average accuracy."
      ],
      "metadata": {
        "id": "NcF_2OZEICYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Set up Stratified K-Fold Cross-Validation\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Step 3: Initialize Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200, multi_class='ovr')\n",
        "\n",
        "# Step 4: Perform Stratified K-Fold Cross-Validation\n",
        "accuracies = []\n",
        "for train_index, test_index in kf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train the model\n",
        "    log_reg.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = log_reg.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy for this fold\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "# Step 5: Calculate the average accuracy\n",
        "average_accuracy = np.mean(accuracies)\n",
        "print(f\"Average Accuracy from Stratified K-Fold Cross-Validation: {average_accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJFfdqjiH9rz",
        "outputId": "d4b82c3c-948b-4749-a60a-6c4a08901512"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy from Stratified K-Fold Cross-Validation: 94.67%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "accuracy.\n"
      ],
      "metadata": {
        "id": "dai3RGteILII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the dataset from a CSV file\n",
        "\n",
        "df = pd.read_csv('advertising.csv')\n",
        "\n",
        "# Step 2: Preprocess the data\n",
        "# For demonstration purposes, we assume the dataset has 'target' as the target variable.\n",
        "# You may need to adjust the following line depending on the dataset's structure.\n",
        "X = df.drop(columns='target')  # Features (all columns except 'target')\n",
        "y = df['target']  # Target variable (assumed to be in the 'target' column)\n",
        "\n",
        "# Step 3: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 4: Train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "K6D39u6jIHYa",
        "outputId": "4ba69c21-84c5-4097-8b2b-7e58ed8ce1d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['target'] not found in axis\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-94b8e89e36dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# For demonstration purposes, we assume the dataset has 'target' as the target variable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# You may need to adjust the following line depending on the dataset's structure.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Features (all columns except 'target')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# Target variable (assumed to be in the 'target' column)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5579\u001b[0m                 \u001b[0mweight\u001b[0m  \u001b[0;36m1.0\u001b[0m     \u001b[0;36m0.8\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5580\u001b[0m         \"\"\"\n\u001b[0;32m-> 5581\u001b[0;31m         return super().drop(\n\u001b[0m\u001b[1;32m   5582\u001b[0m             \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5583\u001b[0m             \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4787\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4788\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4790\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4829\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4830\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4831\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_axis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4832\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7069\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask].tolist()} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7071\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7072\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['target'] not found in axis\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "Logistic Regression. Print the best parameters and accuracy.\n"
      ],
      "metadata": {
        "id": "hZ4kY4VPJXMF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Set up the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200, multi_class='ovr')\n",
        "\n",
        "# Step 4: Define the hyperparameter space for RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'C': np.logspace(-4, 4, 20),  # Regularization strength (log scale)\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],  # Types of regularization\n",
        "    'solver': ['liblinear', 'saga']  # Solvers that support the penalties\n",
        "}\n",
        "\n",
        "# Step 5: Set up RandomizedSearchCV with cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=log_reg, param_distributions=param_dist,\n",
        "                                   n_iter=100, cv=5, verbose=1, random_state=42, n_jobs=-1)\n",
        "\n",
        "# Step 6: Fit the RandomizedSearchCV model\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Print the best hyperparameters found by RandomizedSearchCV\n",
        "print(f\"Best Hyperparameters: {random_search.best_params_}\")\n",
        "\n",
        "# Step 8: Train the best model and make predictions\n",
        "best_model = random_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Step 9: Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Best Model Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-FOe1i5IQOl",
        "outputId": "a47e7bc3-9f82-4bc2-d358-31724f36a367"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
            "Best Hyperparameters: {'solver': 'saga', 'penalty': 'l1', 'C': np.float64(1.623776739188721)}\n",
            "Best Model Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "170 fits failed out of a total of 500.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "85 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1203, in fit\n",
            "    raise ValueError(\"l1_ratio must be specified when penalty is elasticnet.\")\n",
            "ValueError: l1_ratio must be specified when penalty is elasticnet.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "85 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 1193, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py\", line 71, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [0.78095238        nan        nan 0.95238095 0.64761905        nan\n",
            " 0.94285714        nan        nan        nan 0.2952381  0.95238095\n",
            "        nan 0.35238095        nan 0.94285714 0.94285714 0.2952381\n",
            "        nan 0.94285714        nan 0.86666667 0.35238095        nan\n",
            " 0.67619048 0.2952381  0.46666667 0.94285714        nan 0.94285714\n",
            " 0.2952381  0.94285714        nan 0.94285714 0.94285714 0.94285714\n",
            " 0.35238095        nan        nan 0.35238095 0.8        0.33333333\n",
            " 0.94285714        nan 0.95238095 0.94285714        nan 0.94285714\n",
            " 0.94285714 0.83809524 0.86666667        nan        nan 0.93333333\n",
            "        nan        nan 0.33333333 0.84761905 0.94285714 0.94285714\n",
            "        nan 0.78095238 0.36190476        nan        nan 0.35238095\n",
            " 0.35238095 0.94285714 0.34285714        nan 0.66666667 0.94285714\n",
            " 0.2952381         nan 0.94285714        nan 0.94285714 0.8952381\n",
            " 0.94285714        nan 0.94285714 0.96190476 0.94285714 0.94285714\n",
            " 0.94285714        nan        nan 0.84761905        nan 0.95238095\n",
            " 0.94285714 0.64761905        nan        nan 0.95238095 0.94285714\n",
            " 0.87619048        nan 0.94285714 0.32380952]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracyM\n"
      ],
      "metadata": {
        "id": "YEMtb-dGJjxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset (or any other dataset)\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Set up the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Step 4: Implement One-vs-One Multiclass classification using OneVsOneClassifier\n",
        "ovo_classifier = OneVsOneClassifier(log_reg)\n",
        "\n",
        "# Step 5: Train the model\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions on the test set\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "# Step 7: Calculate and print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"One-vs-One Multiclass Logistic Regression Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqQbiXAMJgQP",
        "outputId": "a9540256-89de-4dcf-a9c1-5577d209ee29"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-vs-One Multiclass Logistic Regression Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "classification.\n"
      ],
      "metadata": {
        "id": "33jKvB9gJtCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset and filter it for binary classification\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# For binary classification, we only take the first two classes (0 and 1)\n",
        "X_binary = X[y != 2]  # Only include class 0 and 1\n",
        "y_binary = y[y != 2]  # Only include class 0 and 1\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 5: Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Step 6: Visualize the confusion matrix\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()\n",
        "\n",
        "# Step 7: Print the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "cH3UV_PuJo8d",
        "outputId": "2c433a55-c9a8-4e47-ecdf-bee72d4da437"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "Recall, and F1-Score."
      ],
      "metadata": {
        "id": "Uy9hcC5qJ08-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "\n",
        "# Step 1: Load the Iris dataset and filter it for binary classification\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# For binary classification, we only take the first two classes (0 and 1)\n",
        "X_binary = X[y != 2]  # Only include class 0 and 1\n",
        "y_binary = y[y != 2]  # Only include class 0 and 1\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Train the Logistic Regression model\n",
        "log_reg = LogisticRegression(max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 4: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 5: Compute Precision, Recall, and F1-Score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 6: Print the evaluation metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "\n",
        "# Optionally, you can also print the accuracy:\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tsc3Cg34JxPF",
        "outputId": "42f4a36d-70c5-48cf-bd81-e1c41e0b0d34"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "improve model performance."
      ],
      "metadata": {
        "id": "fKt6C_aZJ9af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Step 1: Load the Iris dataset and create an imbalance for binary classification\n",
        "iris = load_iris()\n",
        "X = iris.data  # Features\n",
        "y = iris.target  # Target labels\n",
        "\n",
        "# For binary classification, we take only two classes (0 and 1)\n",
        "X_binary = X[y != 2]  # Only include class 0 and 1\n",
        "y_binary = y[y != 2]  # Only include class 0 and 1\n",
        "\n",
        "# Create class imbalance by removing a few samples from the minority class (class 1)\n",
        "X_binary = np.vstack([X_binary, X_binary[y_binary == 1][:5]])  # Add extra class 1 samples to imbalance it\n",
        "y_binary = np.hstack([y_binary, np.ones(5)])  # Add extra class 1 labels\n",
        "\n",
        "# Step 2: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_binary, y_binary, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 3: Compute class weights to handle the imbalance\n",
        "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "class_weight_dict = {0: class_weights[0], 1: class_weights[1]}\n",
        "\n",
        "# Step 4: Train the Logistic Regression model with class weights\n",
        "log_reg = LogisticRegression(class_weight=class_weight_dict, max_iter=200)\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Step 5: Make predictions on the test set\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Step 6: Evaluate the model using Precision, Recall, F1-Score, and Accuracy\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 7: Print the evaluation metrics\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd46BKsxJ6dV",
        "outputId": "a926d01d-0506-4193-a5e8-97791e849de2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.00\n",
            "Recall: 1.00\n",
            "F1-Score: 1.00\n",
            "Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "evaluate performance.\n"
      ],
      "metadata": {
        "id": "Y9jrY_LcKGnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values\n",
        "# - For numeric columns, use median to fill missing values\n",
        "# - For categorical columns, use the most frequent value to fill missing values\n",
        "numeric_features = df.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = df.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define imputers for numeric and categorical columns\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Step 3: Preprocess the data\n",
        "# Convert 'Sex' and 'Embarked' columns to numerical values using OneHotEncoding\n",
        "# Drop 'Name' and 'Ticket' columns for simplicity (as they might not be very useful)\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Encode 'Sex' and 'Embarked' using OneHotEncoder\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', OneHotEncoder(), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 4: Define feature matrix X and target vector y\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Create a pipeline that includes preprocessing and model training\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 7: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 10: Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "zFMbvtE5KCc0",
        "outputId": "957adc21-973a-413b-a9a0-b8f562cafba7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "A given column is not a column of the dataframe",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Survived'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    363\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m                 \u001b[0mcol_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Survived'",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-aa12b1486cd8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Step 7: Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Step 8: Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    655\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m             )\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    589\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_column_callables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_remainder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/compose/_column_transformer.py\u001b[0m in \u001b[0;36m_validate_column_callables\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0mall_columns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m             \u001b[0mtransformer_to_input_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_column_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_indexing.py\u001b[0m in \u001b[0;36m_get_column_indices\u001b[0;34m(X, key)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"A given column is not a column of the dataframe\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcolumn_indices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: A given column is not a column of the dataframe"
          ]
        }
      ]
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Define feature matrix X and target vector y\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 3: Handle missing values\n",
        "# - For numeric columns, use median to fill missing values\n",
        "# - For categorical columns, use the most frequent value to fill missing values\n",
        "# Define numeric and categorical features *after* dropping the target variable\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define imputers for numeric and categorical columns\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Step 4: Preprocess the data\n",
        "# Convert 'Sex' and 'Embarked' columns to numerical values using OneHotEncoding\n",
        "# Drop 'Name' and 'Ticket' columns for simplicity (as they might not be very useful)\n",
        "X = X.drop(columns=['Name']) # Drop 'Name' and 'Ticket' from X\n",
        "\n",
        "# Update categorical features after dropping columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Encode 'Sex' and 'Embarked' using OneHotEncoder\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # Added handle_unknown='ignore' for robustness\n",
        "    ])\n",
        "\n",
        "# Step 5: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Create a pipeline that includes preprocessing and model training\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 7: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "# Step 10: Print the evaluation metrics\n",
        "print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1-Score: {f1:.2f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z6QhszWsUeMO",
        "outputId": "9c623303-577c-49cf-b17d-a8d9f109e652"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 76.78%\n",
            "Precision: 0.75\n",
            "Recall: 0.57\n",
            "F1-Score: 0.65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "model. Evaluate its accuracy and compare results with and without scaling."
      ],
      "metadata": {
        "id": "QJvnwJKnKTF9"
      }
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Define feature matrix X and target vector y\n",
        "# Separate the target variable 'Survived' from the features\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 3: Handle missing values and preprocess the data\n",
        "# Drop irrelevant columns (Name, Ticket) - Do this *after* separating X and y\n",
        "X = X.drop(columns=['Name']) # Remove 'Ticket' column as well if not needed, based on previous code\n",
        "\n",
        "# Define numeric and categorical features *after* dropping columns\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define imputers for missing values\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "# Added handle_unknown='ignore' for robustness with unseen categories in test set\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "\n",
        "# Step 4: Create a preprocessor using ColumnTransformer\n",
        "# This preprocessor handles both imputation and encoding/scaling for different column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Without scaling: Model without scaling\n",
        "pipeline_without_scaling = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor), # Use the defined preprocessor\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 6: Train the model without feature scaling\n",
        "pipeline_without_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Step 7: Evaluate the model without scaling\n",
        "y_pred_without_scaling = pipeline_without_scaling.predict(X_test)\n",
        "accuracy_without_scaling = accuracy_score(y_test, y_pred_without_scaling)\n",
        "\n",
        "# Step 8: Create a preprocessor that includes scaling for numeric features\n",
        "# Define the transformers within the pipeline for clarity\n",
        "preprocessor_with_scaling = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[('imputer', SimpleImputer(strategy='median')),\n",
        "                                 ('scaler', StandardScaler())]), numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
        "    ])\n",
        "\n",
        "# Create a pipeline with scaling and Logistic Regression\n",
        "pipeline_with_scaling = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor_with_scaling),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 9: Train the model with feature scaling\n",
        "pipeline_with_scaling.fit(X_train, y_train)\n",
        "\n",
        "# Step 10: Evaluate the model with scaling\n",
        "y_pred_with_scaling = pipeline_with_scaling.predict(X_test)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "# Step 11: Print the results\n",
        "print(f\"Accuracy without scaling: {accuracy_without_scaling * 100:.2f}%\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling * 100:.2f}%\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3UY3aC-UuN2",
        "outputId": "b7e9ac77-abf7-4183-b770-96ddbefc265f"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 76.78%\n",
            "Accuracy with scaling: 76.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score."
      ],
      "metadata": {
        "id": "lwuLaD9sKsdg"
      }
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt # Import matplotlib for plotting\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Define feature matrix X and target vector y\n",
        "# Define feature matrix X and target vector y here, *before* any preprocessing\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 3: Handle missing values and preprocess the data\n",
        "# Drop irrelevant columns (Name, Ticket) from X\n",
        "X = X.drop(columns=['Name']) # Remove 'Ticket' column as well if not needed, based on previous code\n",
        "\n",
        "# Define numeric and categorical features *after* dropping columns from X\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define imputers for missing values\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "# Added handle_unknown='ignore' for robustness with unseen categories in test set\n",
        "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
        "\n",
        "# Step 4: Create a preprocessor using ColumnTransformer\n",
        "# This preprocessor handles both imputation and encoding/scaling for different column types\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "# Use the defined X and y for splitting\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Create the pipeline for Logistic Regression\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 7: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_prob = pipeline.predict_proba(X_test)[:, 1]  # Probabilities for the positive class\n",
        "\n",
        "# Step 9: Evaluate the model using ROC-AUC\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "# Step 10: Print the ROC-AUC score\n",
        "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
        "\n",
        "# Step 11: Plot the ROC Curve (optional)\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic Curve')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "jLzvYqfIVZEv",
        "outputId": "0fdb943b-2866-4950-c612-adba7a7889c0"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.8207\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "accuracy."
      ],
      "metadata": {
        "id": "vVSKolU3K47I"
      }
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values and preprocess the data\n",
        "# Drop irrelevant columns (Name, Ticket - though only Name was dropped in the failing code)\n",
        "# Let's drop Ticket as well, as it was dropped in the successful attempts.\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Step 4: Define feature matrix X and target vector y\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Define numeric and categorical features *after* splitting X and y\n",
        "# This ensures 'Survived' is not included in feature lists for X\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define imputers for missing values\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Step 3: Preprocess and encode categorical variables\n",
        "# Convert categorical columns to numerical using OneHotEncoder\n",
        "# Update the preprocessor to use the correct feature lists\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # Added handle_unknown for robustness\n",
        "    ])\n",
        "\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression model with a custom learning rate (C=0.5)\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(C=0.5, max_iter=200, solver='lbfgs'))\n",
        "])\n",
        "\n",
        "# Step 7: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model using accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 10: Print the accuracy\n",
        "print(f\"Accuracy with C=0.5: {accuracy * 100:.2f}%\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnOzb_ytTZCp",
        "outputId": "32997948-99dc-4ac9-a9ac-5fc2c9fd82ff"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with C=0.5: 76.78%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "coefficients."
      ],
      "metadata": {
        "id": "C2UEu0KBK9e8"
      }
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values and preprocess the data\n",
        "# Drop irrelevant columns (Name)\n",
        "df = df.drop(columns=['Name']) # Keep 'Ticket' for potential future use or dropping after splitting\n",
        "\n",
        "# Step 4: Define feature matrix X and target vector y *BEFORE* defining features for preprocessing\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "\n",
        "# Define numeric and categorical features based on X\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define imputers for missing values\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Step 3: Preprocess and encode categorical variables\n",
        "# Convert categorical columns to numerical using OneHotEncoder\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # Added handle_unknown='ignore' for robustness\n",
        "    ])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Create the pipeline for Logistic Regression\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 7: Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Evaluate the model's accuracy\n",
        "y_pred = pipeline.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 9: Extract model coefficients\n",
        "# Need to get feature names from the fitted preprocessor\n",
        "fitted_preprocessor = pipeline.named_steps['preprocessor']\n",
        "\n",
        "# Get names of transformed columns\n",
        "transformed_features = []\n",
        "for name, trans, cols in fitted_preprocessor.transformers_:\n",
        "    if trans == 'passthrough':\n",
        "        transformed_features.extend(cols)\n",
        "    elif hasattr(trans, 'get_feature_names_out'):\n",
        "        transformed_features.extend(trans.get_feature_names_out(cols))\n",
        "    else:\n",
        "        # For imputers or other simple transformers that don't change shape/names\n",
        "        transformed_features.extend(cols) # This might need adjustment depending on transformers\n",
        "\n",
        "model = pipeline.named_steps['classifier']\n",
        "coefficients = model.coef_[0]  # Coefficients of the model\n",
        "\n",
        "# Step 10: Extract feature names (including encoded ones) - Updated to use transformed_features\n",
        "feature_names = transformed_features\n",
        "\n",
        "# Step 11: Create a DataFrame of feature importance\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Coefficient': coefficients\n",
        "})\n",
        "\n",
        "# Step 12: Sort features by absolute coefficient value\n",
        "feature_importance['Abs_Coefficient'] = feature_importance['Coefficient'].abs()\n",
        "feature_importance = feature_importance.sort_values(by='Abs_Coefficient', ascending=False)\n",
        "\n",
        "# Step 13: Print accuracy and feature importance\n",
        "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
        "print(\"\\nFeature Importance based on coefficients:\")\n",
        "# Only print top N features for better readability if there are many\n",
        "print(feature_importance[['Feature', 'Coefficient']].head(15)) # Print top 15\n",
        "\n",
        "# Step 14: Plot the feature importance\n",
        "plt.figure(figsize=(10, len(feature_importance) * 0.3)) # Adjust figure size based on number of features\n",
        "plt.barh(feature_importance['Feature'], feature_importance['Abs_Coefficient'], color='skyblue')\n",
        "plt.xlabel('Absolute Coefficient Value')\n",
        "plt.title('Feature Importance based on Logistic Regression Coefficients')\n",
        "plt.gca().invert_yaxis() # Show most important features at the top\n",
        "plt.tight_layout() # Adjust layout\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "rA9dkdqESfsD",
        "outputId": "7945ab21-e630-4793-c187-400d7d8a05fa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Accuracy: 76.78%\n",
            "\n",
            "Feature Importance based on coefficients:\n",
            "                   Feature  Coefficient\n",
            "5               Sex_female     1.429577\n",
            "6                 Sex_male    -1.428405\n",
            "0                   Pclass    -1.136357\n",
            "2  Siblings/Spouses Aboard    -0.494303\n",
            "3  Parents/Children Aboard    -0.113903\n",
            "1                      Age    -0.041441\n",
            "4                     Fare     0.002268\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x210 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "19.  Write a Python program to train Logistic Regression and evaluate its performance using Cohen‚Äôs Kappa\n",
        "Score."
      ],
      "metadata": {
        "id": "tMjwUcY0LMTa"
      }
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Handle missing values and preprocess the data\n",
        "# Drop irrelevant columns (Name)\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Step 3: Define feature matrix X and target vector y\n",
        "# Define feature matrix X by dropping the target column\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Define numeric and categorical features AFTER splitting X and y\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define imputers for missing values\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Step 4: Preprocess and encode categorical variables\n",
        "# Convert categorical columns to numerical using OneHotEncoder\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # Added handle_unknown='ignore' for robustness\n",
        "    ])\n",
        "\n",
        "# Step 5: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 6: Train Logistic Regression model\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 7: Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 8: Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Step 9: Evaluate the model using Cohen's Kappa Score\n",
        "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
        "\n",
        "# Step 10: Print the Cohen's Kappa score\n",
        "print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLWGoqEgSJQE",
        "outputId": "218a05bd-df0d-4b53-b9f0-268e7933941c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cohen's Kappa Score: 0.4822\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "classificatio."
      ],
      "metadata": {
        "id": "uy0qy_K1LYjd"
      }
    },
    {
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Load the Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Preprocess the data (handle missing values and encode categorical variables)\n",
        "# Drop irrelevant columns (Name, Ticket)\n",
        "# Make sure to drop both 'Name' and 'Ticket' consistently\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Step 3: Define feature matrix X and target vector y\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Define numeric and categorical features AFTER creating X\n",
        "numeric_features = X.select_dtypes(include=['float64', 'int64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Define imputers for missing values\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "# Preprocess and encode categorical variables\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numeric_transformer, numeric_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features) # Added handle_unknown='ignore' for robustness\n",
        "    ])\n",
        "\n",
        "# Step 4: Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 5: Train Logistic Regression model\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Fit the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 6: Make predictions and get the probability scores\n",
        "y_probs = pipeline.predict_proba(X_test)[:, 1]  # Get the probabilities for the positive class (Survived=1)\n",
        "\n",
        "# Step 7: Generate Precision-Recall Curve\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_probs)\n",
        "\n",
        "# Step 8: Plot Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, color='b', label=f'Precision-Recall Curve (AUC = {auc(recall, precision):.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "StVxiGk9RmUm",
        "outputId": "57f4aab7-3fb6-4107-c05e-2cc9790e5516"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "21.Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "their accuracy"
      ],
      "metadata": {
        "id": "CReFhVlhMAB3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Drop irrelevant columns\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Step 3: Define feature and target variables\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 4: Define numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Step 5: Preprocessing pipelines\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Step 6: Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Try different solvers\n",
        "solvers = ['liblinear', 'saga', 'lbfgs']\n",
        "results = {}\n",
        "\n",
        "for solver in solvers:\n",
        "    try:\n",
        "        pipeline = Pipeline(steps=[\n",
        "            ('preprocessor', preprocessor),\n",
        "            ('classifier', LogisticRegression(solver=solver, max_iter=200))\n",
        "        ])\n",
        "        pipeline.fit(X_train, y_train)\n",
        "        y_pred = pipeline.predict(X_test)\n",
        "        acc = accuracy_score(y_test, y_pred)\n",
        "        results[solver] = acc\n",
        "    except Exception as e:\n",
        "        results[solver] = f\"Error: {e}\"\n",
        "\n",
        "# Step 8: Display results\n",
        "print(\"Accuracy Comparison of Different Solvers:\")\n",
        "for solver, acc in results.items():\n",
        "    print(f\"{solver}: {acc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smaTfGIwL_ok",
        "outputId": "9eee1db0-7423-4e33-89eb-a689ae570671"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy Comparison of Different Solvers:\n",
            "liblinear: 0.7677902621722846\n",
            "saga: 0.6816479400749064\n",
            "lbfgs: 0.7677902621722846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "Correlation Coefficient (MCC)"
      ],
      "metadata": {
        "id": "Yox2V2-aMELN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Drop irrelevant columns\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Step 3: Split into features and target\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 4: Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Step 5: Preprocessing\n",
        "numeric_transformer = SimpleImputer(strategy='median')\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Step 6: Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Build pipeline with Logistic Regression\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 8: Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Predict on test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Step 10: Evaluate with Matthews Correlation Coefficient\n",
        "mcc = matthews_corrcoef(y_test, y_pred)\n",
        "\n",
        "# Step 11: Output the MCC\n",
        "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LA9FosS3MF6f",
        "outputId": "44f5f7a9-15d7-4c41-e7b8-f62ca5998ce3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matthews Correlation Coefficient (MCC): 0.4922\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "accuracy to see the impact of feature scaling."
      ],
      "metadata": {
        "id": "1iTqFmH_MGVo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Drop irrelevant columns\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Step 3: Split into features and target\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 4: Identify numeric and categorical features\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Step 5: Define transformers\n",
        "numeric_imputer = SimpleImputer(strategy='median')\n",
        "categorical_pipeline = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "# Pipelines for raw and standardized data\n",
        "# Raw data pipeline\n",
        "raw_preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_imputer, numeric_features),\n",
        "    ('cat', categorical_pipeline, categorical_features)\n",
        "])\n",
        "\n",
        "# Standardized data pipeline\n",
        "scaled_preprocessor = ColumnTransformer([\n",
        "    ('num', Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ]), numeric_features),\n",
        "    ('cat', categorical_pipeline, categorical_features)\n",
        "])\n",
        "\n",
        "# Step 6: Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Define pipelines\n",
        "# Raw pipeline\n",
        "pipeline_raw = Pipeline(steps=[\n",
        "    ('preprocessor', raw_preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Scaled pipeline\n",
        "pipeline_scaled = Pipeline(steps=[\n",
        "    ('preprocessor', scaled_preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 8: Train both models\n",
        "pipeline_raw.fit(X_train, y_train)\n",
        "pipeline_scaled.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Make predictions\n",
        "y_pred_raw = pipeline_raw.predict(X_test)\n",
        "y_pred_scaled = pipeline_scaled.predict(X_test)\n",
        "\n",
        "# Step 10: Evaluate and compare accuracy\n",
        "acc_raw = accuracy_score(y_test, y_pred_raw)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "# Step 11: Print results\n",
        "print(f\"Accuracy without Standardization: {acc_raw:.4f}\")\n",
        "print(f\"Accuracy with Standardization:    {acc_scaled:.4f}\")\n",
        "\n",
        "# Optional: Highlight improvement\n",
        "if acc_scaled > acc_raw:\n",
        "    print(\"‚úÖ Standardization improved model accuracy.\")\n",
        "elif acc_scaled < acc_raw:\n",
        "    print(\"‚ö†Ô∏è  Standardization reduced model accuracy.\")\n",
        "else:\n",
        "    print(\"‚ûñ No difference in accuracy with standardization.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nObug8MDMHwh",
        "outputId": "7328c6b6-b968-4ce9-f912-c10afdcb74e0"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without Standardization: 0.7678\n",
            "Accuracy with Standardization:    0.7678\n",
            "‚ûñ No difference in accuracy with standardization.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "cross-validation."
      ],
      "metadata": {
        "id": "jXNCYSj6MIHj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Step 1: Load dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Drop irrelevant columns\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Step 3: Features and target\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 4: Identify feature types\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Step 5: Define preprocessors\n",
        "numeric_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Step 6: Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 7: Full pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200, solver='liblinear'))  # liblinear supports all Cs\n",
        "])\n",
        "\n",
        "# Step 8: Grid search for best C\n",
        "param_grid = {\n",
        "    'classifier__C': [0.01, 0.1, 1, 10, 100]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Best model and evaluation\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Step 10: Print results\n",
        "print(f\"Best C value: {grid_search.best_params_['classifier__C']}\")\n",
        "print(f\"Cross-validated training accuracy: {grid_search.best_score_:.4f}\")\n",
        "print(f\"Test accuracy with best C: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JMdbN1dMJhN",
        "outputId": "f89849fe-547d-4c1f-f526-26d50646b956"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best C value: 0.1\n",
            "Cross-validated training accuracy: 0.8226\n",
            "Test accuracy with best C: 0.7753\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "make predictions."
      ],
      "metadata": {
        "id": "sn8Z2aiKMJ0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import accuracy_score\n",
        "import joblib\n",
        "\n",
        "# Step 1: Load Titanic dataset\n",
        "url = 'https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/stuff/titanic.csv'\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Step 2: Drop irrelevant columns\n",
        "df = df.drop(columns=['Name'])\n",
        "\n",
        "# Step 3: Features and target\n",
        "X = df.drop(columns=['Survived'])\n",
        "y = df['Survived']\n",
        "\n",
        "# Step 4: Identify numeric and categorical columns\n",
        "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "categorical_features = X.select_dtypes(include=['object']).columns\n",
        "\n",
        "# Step 5: Preprocessing pipeline\n",
        "numeric_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline([\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', numeric_transformer, numeric_features),\n",
        "    ('cat', categorical_transformer, categorical_features)\n",
        "])\n",
        "\n",
        "# Step 6: Create pipeline with Logistic Regression\n",
        "model = Pipeline([\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=200))\n",
        "])\n",
        "\n",
        "# Step 7: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 8: Train the model\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Step 9: Save the trained model\n",
        "joblib.dump(model, 'logistic_model.joblib')\n",
        "print(\"‚úÖ Model saved to 'logistic_model.joblib'\")\n",
        "\n",
        "# Step 10: Load the model from disk\n",
        "loaded_model = joblib.load('logistic_model.joblib')\n",
        "print(\"üîÑ Model loaded from disk\")\n",
        "\n",
        "# Step 11: Make predictions and evaluate\n",
        "y_pred = loaded_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"üìä Accuracy of loaded model: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYqW32XGMYQ2",
        "outputId": "4b57a220-5b27-4ea3-d988-dbbc93128604"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model saved to 'logistic_model.joblib'\n",
            "üîÑ Model loaded from disk\n",
            "üìä Accuracy of loaded model: 0.7678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYphc-r8OH2p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}